{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jfz1nZKW5j8m"
   },
   "source": [
    "## Importacion de los modulos necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20662,
     "status": "ok",
     "timestamp": 1693252353393,
     "user": {
      "displayName": "ShadowMan",
      "userId": "06933040393855637082"
     },
     "user_tz": 420
    },
    "id": "rgGYzjdFPesN",
    "outputId": "c5541e97-6f93-4239-cd6a-83699742d5c9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "# Procesamiento de datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import utils\n",
    "from sklearn import preprocessing as prp\n",
    "import shap\n",
    "import optuna\n",
    "\n",
    "# Modelos\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Graficadores\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "#Otros\n",
    "# !pip install category_encoders\n",
    "from category_encoders import TargetEncoder\n",
    "# !pip install scikeras\n",
    "from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4mIdIGe5qSA"
   },
   "source": [
    "## Cargar y modificar los nombres de las columnas del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1109,
     "status": "ok",
     "timestamp": 1693252354498,
     "user": {
      "displayName": "ShadowMan",
      "userId": "06933040393855637082"
     },
     "user_tz": 420
    },
    "id": "kqg_JL48Pv18",
    "outputId": "482f82cd-9bf6-4088-932f-90c29bc174cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(342, 18)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_excel(\"DF_D.xlsx\")\n",
    "names = ['label','edad','genero','ANB','ICS/SN','IMPA','ICI/PLANO','interincisal','lab sup/lin E','lab inf/lin E','nasolabial','Overjet','Oberbite','j-s','Angulo','Discrepancia','Clase','Perfil']\n",
    "df = pd.DataFrame(dataset)\n",
    "df = df.drop(['No.'],axis=1)\n",
    "df = df.drop(0,axis=0)\n",
    "df.columns = names\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmVDO45K54Gk"
   },
   "source": [
    "### Se extrae un valor que funje como outlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ja3EulaVSW5q"
   },
   "outputs": [],
   "source": [
    "# Se elimina la columna con el outlayer de 932\n",
    "df = df.drop(df[df['j-s']==931].index[0],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGXrlRDt59eC"
   },
   "source": [
    "## Se ve la integridad y naturaleza estadistica de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1693252354500,
     "user": {
      "displayName": "ShadowMan",
      "userId": "06933040393855637082"
     },
     "user_tz": 420
    },
    "id": "_tImqM5wYfqb",
    "outputId": "40a1a83f-6107-4d90-8ebb-e9bf69eaa529"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label             2\n",
       "edad             32\n",
       "genero            4\n",
       "ANB              24\n",
       "ICS/SN           39\n",
       "IMPA             42\n",
       "ICI/PLANO        39\n",
       "interincisal     53\n",
       "lab sup/lin E    24\n",
       "lab inf/lin E    27\n",
       "nasolabial       55\n",
       "Overjet          27\n",
       "Oberbite         24\n",
       "j-s              32\n",
       "Angulo           31\n",
       "Discrepancia      5\n",
       "Clase             3\n",
       "Perfil            5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()\n",
    "# 4 generos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1693252354501,
     "user": {
      "displayName": "ShadowMan",
      "userId": "06933040393855637082"
     },
     "user_tz": 420
    },
    "id": "jNIFygHvZRir",
    "outputId": "31f80d6d-501f-4cbf-fa6d-f4cda711693d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M' 'F' 'f ' 'f']\n",
      "['M' 'F']\n"
     ]
    }
   ],
   "source": [
    "print(df.genero.unique())\n",
    "df.genero = df.genero.replace({'f ':'F'})\n",
    "df.genero = df.genero.replace({'f':'F'})\n",
    "print(df.genero.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1693252354501,
     "user": {
      "displayName": "ShadowMan",
      "userId": "06933040393855637082"
     },
     "user_tz": 420
    },
    "id": "BVzjsZNSa63x",
    "outputId": "57cdef2c-9023-4d21-b8d4-fcf708203507"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 341 entries, 1 to 342\n",
      "Data columns (total 18 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   label          341 non-null    float64\n",
      " 1   edad           341 non-null    float64\n",
      " 2   genero         341 non-null    object \n",
      " 3   ANB            341 non-null    float64\n",
      " 4   ICS/SN         341 non-null    float64\n",
      " 5   IMPA           341 non-null    float64\n",
      " 6   ICI/PLANO      341 non-null    float64\n",
      " 7   interincisal   341 non-null    float64\n",
      " 8   lab sup/lin E  341 non-null    float64\n",
      " 9   lab inf/lin E  341 non-null    float64\n",
      " 10  nasolabial     341 non-null    float64\n",
      " 11  Overjet        341 non-null    float64\n",
      " 12  Oberbite       341 non-null    float64\n",
      " 13  j-s            341 non-null    float64\n",
      " 14  Angulo         341 non-null    float64\n",
      " 15  Discrepancia   341 non-null    float64\n",
      " 16  Clase          341 non-null    float64\n",
      " 17  Perfil         341 non-null    float64\n",
      "dtypes: float64(17), object(1)\n",
      "memory usage: 50.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmYBYw6L6HrS"
   },
   "source": [
    "## Se mezclan las columnas del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1693252354501,
     "user": {
      "displayName": "ShadowMan",
      "userId": "06933040393855637082"
     },
     "user_tz": 420
    },
    "id": "uBRATsLEYMeG",
    "outputId": "b849a5f0-3715-4d41-c835-9134faec8f11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((272, 17), (69, 17))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = utils.shuffle(df,random_state=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.drop(['label'],axis=1),df['label'],test_size=0.2,random_state=42)\n",
    "x_train.shape,x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTQErm586Rp5"
   },
   "source": [
    "## Realizacion de la codificacion de variables categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 217,
     "status": "ok",
     "timestamp": 1693252354712,
     "user": {
      "displayName": "ShadowMan",
      "userId": "06933040393855637082"
     },
     "user_tz": 420
    },
    "id": "EZpSFci0dvoO",
    "outputId": "fadd05cd-b3b0-454c-8f1f-a0ea2fc64589"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 272 entries, 172 to 79\n",
      "Data columns (total 17 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   edad           272 non-null    float64\n",
      " 1   genero         272 non-null    float64\n",
      " 2   ANB            272 non-null    float64\n",
      " 3   ICS/SN         272 non-null    float64\n",
      " 4   IMPA           272 non-null    float64\n",
      " 5   ICI/PLANO      272 non-null    float64\n",
      " 6   interincisal   272 non-null    float64\n",
      " 7   lab sup/lin E  272 non-null    float64\n",
      " 8   lab inf/lin E  272 non-null    float64\n",
      " 9   nasolabial     272 non-null    float64\n",
      " 10  Overjet        272 non-null    float64\n",
      " 11  Oberbite       272 non-null    float64\n",
      " 12  j-s            272 non-null    float64\n",
      " 13  Angulo         272 non-null    float64\n",
      " 14  Discrepancia   272 non-null    float64\n",
      " 15  Clase          272 non-null    float64\n",
      " 16  Perfil         272 non-null    float64\n",
      "dtypes: float64(17)\n",
      "memory usage: 38.2 KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edad</th>\n",
       "      <th>genero</th>\n",
       "      <th>ANB</th>\n",
       "      <th>ICS/SN</th>\n",
       "      <th>IMPA</th>\n",
       "      <th>ICI/PLANO</th>\n",
       "      <th>interincisal</th>\n",
       "      <th>lab sup/lin E</th>\n",
       "      <th>lab inf/lin E</th>\n",
       "      <th>nasolabial</th>\n",
       "      <th>Overjet</th>\n",
       "      <th>Oberbite</th>\n",
       "      <th>j-s</th>\n",
       "      <th>Angulo</th>\n",
       "      <th>Discrepancia</th>\n",
       "      <th>Clase</th>\n",
       "      <th>Perfil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.489150</td>\n",
       "      <td>3.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.718648</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>29.0</td>\n",
       "      <td>0.489150</td>\n",
       "      <td>3.5</td>\n",
       "      <td>106.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>400.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.386076</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>8.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.718648</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>16.0</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>3.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>402.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.386076</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>7.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>394.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.386076</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     edad    genero  ANB  ICS/SN  IMPA  ICI/PLANO  interincisal  \\\n",
       "172  15.0  0.489150  3.0   102.0  75.0       91.0         125.0   \n",
       "148  29.0  0.489150  3.5   106.0  66.0      106.0         110.0   \n",
       "209  12.0  0.527778  8.0   119.0  51.0       92.0         105.0   \n",
       "179  16.0  0.527778  3.0   110.0  62.0      100.0         108.0   \n",
       "282  21.0  0.527778  7.0   100.0  73.0      100.0         120.0   \n",
       "\n",
       "     lab sup/lin E  lab inf/lin E  nasolabial  Overjet  Oberbite    j-s  \\\n",
       "172            0.0            1.0        72.0      2.0       2.0  398.0   \n",
       "148           -2.0            0.0        65.0      3.0       2.5  400.0   \n",
       "209            3.5            3.0        96.0     10.0       5.0  401.0   \n",
       "179            0.0            1.0        88.0      5.0       3.5  402.0   \n",
       "282            0.0            0.0       113.0      1.5       1.0  394.0   \n",
       "\n",
       "     Angulo  Discrepancia     Clase  Perfil  \n",
       "172   172.0           4.0  0.718648     2.0  \n",
       "148   169.0           2.0  0.386076     1.0  \n",
       "209   159.0           3.0  0.718648     3.0  \n",
       "179   165.0           4.0  0.386076     2.0  \n",
       "282   163.0           3.0  0.386076     1.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "encoder = TargetEncoder(cols=['genero','Clase'])\n",
    "encoder.fit(x_train,y_train)\n",
    "#Utiliza el mismo encoder para transformar el dataset de testing y el de testing?\n",
    "df_train_enc = encoder.transform(x_train)\n",
    "df_test_enc = encoder.transform(x_test)\n",
    "print(df_train_enc.info())\n",
    "df_train_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wcoN4rNva0sz"
   },
   "outputs": [],
   "source": [
    "# # Testing\n",
    "# encoder = TargetEncoder(cols=['genero','Clase'])\n",
    "# encoder.fit(x_test,y_test)\n",
    "# df_test_enc = encoder.transform(x_test,y_test)\n",
    "# # df_test_enc = df_test_enc.astype(float)\n",
    "# print(df_test_enc.info())\n",
    "# df_test_enc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUMHf5Jl7MAg"
   },
   "source": [
    "## EDA mediante diagramas de violin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4315,
     "status": "ok",
     "timestamp": 1693252359023,
     "user": {
      "displayName": "ShadowMan",
      "userId": "06933040393855637082"
     },
     "user_tz": 420
    },
    "id": "FQOEXPm8bpu7",
    "outputId": "8574330d-f5f5-45eb-8ec6-2742ef57082c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,30))\n",
    "i = 1\n",
    "for column in df_train_enc:\n",
    "  plt.subplot(6,3,i)\n",
    "  plt.violinplot(dataset=df_train_enc[column],vert=False)\n",
    "  plt.title(column)\n",
    "  i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1693252359024,
     "user": {
      "displayName": "ShadowMan",
      "userId": "06933040393855637082"
     },
     "user_tz": 420
    },
    "id": "LrELsfwenZYz",
    "outputId": "805b828d-a22d-4133-c4b5-ccc177ab7659"
   },
   "outputs": [],
   "source": [
    "df_train_enc['ICI/PLANO'].unique()\n",
    "#Valores atipicos de bajo valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Btz1yKkA7jYi"
   },
   "source": [
    "## Estandarizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1693252359025,
     "user": {
      "displayName": "ShadowMan",
      "userId": "06933040393855637082"
     },
     "user_tz": 420
    },
    "id": "foJoFG3fAxQ5",
    "outputId": "1c92bc0d-bde7-4851-cb71-1ade21abe5b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "n_quantiles (1000) is greater than the total number of samples (272). n_quantiles is set to n_samples.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edad</th>\n",
       "      <th>genero</th>\n",
       "      <th>ANB</th>\n",
       "      <th>ICS/SN</th>\n",
       "      <th>IMPA</th>\n",
       "      <th>ICI/PLANO</th>\n",
       "      <th>interincisal</th>\n",
       "      <th>lab sup/lin E</th>\n",
       "      <th>lab inf/lin E</th>\n",
       "      <th>nasolabial</th>\n",
       "      <th>Overjet</th>\n",
       "      <th>Oberbite</th>\n",
       "      <th>j-s</th>\n",
       "      <th>Angulo</th>\n",
       "      <th>Discrepancia</th>\n",
       "      <th>Clase</th>\n",
       "      <th>Perfil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.455720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.265683</td>\n",
       "      <td>0.204797</td>\n",
       "      <td>0.933579</td>\n",
       "      <td>0.191882</td>\n",
       "      <td>0.756458</td>\n",
       "      <td>0.630996</td>\n",
       "      <td>0.568266</td>\n",
       "      <td>0.014760</td>\n",
       "      <td>0.214022</td>\n",
       "      <td>0.370849</td>\n",
       "      <td>0.603321</td>\n",
       "      <td>0.885609</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.608856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.933579</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.341328</td>\n",
       "      <td>0.418819</td>\n",
       "      <td>0.595941</td>\n",
       "      <td>0.894834</td>\n",
       "      <td>0.191882</td>\n",
       "      <td>0.267528</td>\n",
       "      <td>0.369004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.442804</td>\n",
       "      <td>0.518450</td>\n",
       "      <td>0.726937</td>\n",
       "      <td>0.769373</td>\n",
       "      <td>0.256458</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.238007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.138376</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.896679</td>\n",
       "      <td>0.931734</td>\n",
       "      <td>0.036900</td>\n",
       "      <td>0.238007</td>\n",
       "      <td>0.079336</td>\n",
       "      <td>0.961255</td>\n",
       "      <td>0.835793</td>\n",
       "      <td>0.459410</td>\n",
       "      <td>0.977860</td>\n",
       "      <td>0.928044</td>\n",
       "      <td>0.787823</td>\n",
       "      <td>0.118081</td>\n",
       "      <td>0.494465</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.857934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.584871</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.265683</td>\n",
       "      <td>0.660517</td>\n",
       "      <td>0.381919</td>\n",
       "      <td>0.612546</td>\n",
       "      <td>0.142066</td>\n",
       "      <td>0.630996</td>\n",
       "      <td>0.568266</td>\n",
       "      <td>0.169742</td>\n",
       "      <td>0.824723</td>\n",
       "      <td>0.800738</td>\n",
       "      <td>0.832103</td>\n",
       "      <td>0.464945</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.608856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.852399</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.821033</td>\n",
       "      <td>0.118081</td>\n",
       "      <td>0.900369</td>\n",
       "      <td>0.612546</td>\n",
       "      <td>0.586716</td>\n",
       "      <td>0.630996</td>\n",
       "      <td>0.369004</td>\n",
       "      <td>0.939114</td>\n",
       "      <td>0.132841</td>\n",
       "      <td>0.160517</td>\n",
       "      <td>0.352399</td>\n",
       "      <td>0.333948</td>\n",
       "      <td>0.494465</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.238007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>0.044280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.896679</td>\n",
       "      <td>0.948339</td>\n",
       "      <td>0.059041</td>\n",
       "      <td>0.385609</td>\n",
       "      <td>0.079336</td>\n",
       "      <td>0.630996</td>\n",
       "      <td>0.730627</td>\n",
       "      <td>0.208487</td>\n",
       "      <td>0.985240</td>\n",
       "      <td>0.845018</td>\n",
       "      <td>0.726937</td>\n",
       "      <td>0.267528</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.608856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>0.254613</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.418819</td>\n",
       "      <td>0.714022</td>\n",
       "      <td>0.479705</td>\n",
       "      <td>0.817343</td>\n",
       "      <td>0.359779</td>\n",
       "      <td>0.885609</td>\n",
       "      <td>0.568266</td>\n",
       "      <td>0.734317</td>\n",
       "      <td>0.695572</td>\n",
       "      <td>0.370849</td>\n",
       "      <td>0.105166</td>\n",
       "      <td>0.333948</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.608856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>0.684502</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.418819</td>\n",
       "      <td>0.153137</td>\n",
       "      <td>0.595941</td>\n",
       "      <td>0.492620</td>\n",
       "      <td>0.675277</td>\n",
       "      <td>0.145756</td>\n",
       "      <td>0.238007</td>\n",
       "      <td>0.656827</td>\n",
       "      <td>0.308118</td>\n",
       "      <td>0.673432</td>\n",
       "      <td>0.787823</td>\n",
       "      <td>0.267528</td>\n",
       "      <td>0.494465</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.238007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>0.584871</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.577491</td>\n",
       "      <td>0.861624</td>\n",
       "      <td>0.114391</td>\n",
       "      <td>0.771218</td>\n",
       "      <td>0.079336</td>\n",
       "      <td>0.630996</td>\n",
       "      <td>0.835793</td>\n",
       "      <td>0.459410</td>\n",
       "      <td>0.887454</td>\n",
       "      <td>0.160517</td>\n",
       "      <td>0.529520</td>\n",
       "      <td>0.267528</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.857934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>0.254613</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.952030</td>\n",
       "      <td>0.204797</td>\n",
       "      <td>0.538745</td>\n",
       "      <td>0.612546</td>\n",
       "      <td>0.079336</td>\n",
       "      <td>0.835793</td>\n",
       "      <td>0.931734</td>\n",
       "      <td>0.778598</td>\n",
       "      <td>0.042435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.664207</td>\n",
       "      <td>0.175277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>272 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         edad  genero       ANB    ICS/SN      IMPA  ICI/PLANO  interincisal  \\\n",
       "0    0.455720     0.0  0.265683  0.204797  0.933579   0.191882      0.756458   \n",
       "1    0.933579     0.0  0.341328  0.418819  0.595941   0.894834      0.191882   \n",
       "2    0.138376     1.0  0.896679  0.931734  0.036900   0.238007      0.079336   \n",
       "3    0.584871     1.0  0.265683  0.660517  0.381919   0.612546      0.142066   \n",
       "4    0.852399     1.0  0.821033  0.118081  0.900369   0.612546      0.586716   \n",
       "..        ...     ...       ...       ...       ...        ...           ...   \n",
       "267  0.044280     0.0  0.896679  0.948339  0.059041   0.385609      0.079336   \n",
       "268  0.254613     0.0  0.418819  0.714022  0.479705   0.817343      0.359779   \n",
       "269  0.684502     1.0  0.418819  0.153137  0.595941   0.492620      0.675277   \n",
       "270  0.584871     1.0  0.577491  0.861624  0.114391   0.771218      0.079336   \n",
       "271  0.254613     1.0  0.952030  0.204797  0.538745   0.612546      0.079336   \n",
       "\n",
       "     lab sup/lin E  lab inf/lin E  nasolabial   Overjet  Oberbite       j-s  \\\n",
       "0         0.630996       0.568266    0.014760  0.214022  0.370849  0.603321   \n",
       "1         0.267528       0.369004    0.000000  0.442804  0.518450  0.726937   \n",
       "2         0.961255       0.835793    0.459410  0.977860  0.928044  0.787823   \n",
       "3         0.630996       0.568266    0.169742  0.824723  0.800738  0.832103   \n",
       "4         0.630996       0.369004    0.939114  0.132841  0.160517  0.352399   \n",
       "..             ...            ...         ...       ...       ...       ...   \n",
       "267       0.630996       0.730627    0.208487  0.985240  0.845018  0.726937   \n",
       "268       0.885609       0.568266    0.734317  0.695572  0.370849  0.105166   \n",
       "269       0.145756       0.238007    0.656827  0.308118  0.673432  0.787823   \n",
       "270       0.630996       0.835793    0.459410  0.887454  0.160517  0.529520   \n",
       "271       0.835793       0.931734    0.778598  0.042435  0.000000  0.664207   \n",
       "\n",
       "       Angulo  Discrepancia  Clase    Perfil  \n",
       "0    0.885609      1.000000    1.0  0.608856  \n",
       "1    0.769373      0.256458    0.0  0.238007  \n",
       "2    0.118081      0.494465    1.0  0.857934  \n",
       "3    0.464945      1.000000    0.0  0.608856  \n",
       "4    0.333948      0.494465    0.0  0.238007  \n",
       "..        ...           ...    ...       ...  \n",
       "267  0.267528      1.000000    1.0  0.608856  \n",
       "268  0.333948      1.000000    1.0  0.608856  \n",
       "269  0.267528      0.494465    0.0  0.238007  \n",
       "270  0.267528      1.000000    0.0  0.857934  \n",
       "271  0.175277      0.000000    1.0  1.000000  \n",
       "\n",
       "[272 rows x 17 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "# transform = prp.MinMaxScaler()#82\n",
    "# transform = prp.RobustScaler()\n",
    "# transform = prp.StandardScaler()\n",
    "transform = prp.QuantileTransformer(output_distribution='uniform')\n",
    "colums = transform.fit_transform(df_train_enc)\n",
    "df_train_enc_est = pd.DataFrame(colums)\n",
    "df_train_enc_est.columns = df_train_enc.columns\n",
    "df_train_enc_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1693252359234,
     "user": {
      "displayName": "ShadowMan",
      "userId": "06933040393855637082"
     },
     "user_tz": 420
    },
    "id": "rpA16nG4A2M7",
    "outputId": "69c22e66-fc8e-4072-e6cf-b4ad0a424609"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "n_quantiles (1000) is greater than the total number of samples (69). n_quantiles is set to n_samples.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edad</th>\n",
       "      <th>genero</th>\n",
       "      <th>ANB</th>\n",
       "      <th>ICS/SN</th>\n",
       "      <th>IMPA</th>\n",
       "      <th>ICI/PLANO</th>\n",
       "      <th>interincisal</th>\n",
       "      <th>lab sup/lin E</th>\n",
       "      <th>lab inf/lin E</th>\n",
       "      <th>nasolabial</th>\n",
       "      <th>Overjet</th>\n",
       "      <th>Oberbite</th>\n",
       "      <th>j-s</th>\n",
       "      <th>Angulo</th>\n",
       "      <th>Discrepancia</th>\n",
       "      <th>Clase</th>\n",
       "      <th>Perfil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.985294</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.227941</td>\n",
       "      <td>0.213235</td>\n",
       "      <td>0.713235</td>\n",
       "      <td>0.720588</td>\n",
       "      <td>0.720588</td>\n",
       "      <td>0.933824</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.654412</td>\n",
       "      <td>0.713235</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.661765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.198529</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.110294</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.301471</td>\n",
       "      <td>0.661765</td>\n",
       "      <td>0.066176</td>\n",
       "      <td>0.080882</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.595588</td>\n",
       "      <td>0.816176</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.661765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.022059</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.198529</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.051471</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.301471</td>\n",
       "      <td>0.661765</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.419118</td>\n",
       "      <td>0.573529</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.661765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.257353</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.757353</td>\n",
       "      <td>0.110294</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.073529</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.713235</td>\n",
       "      <td>0.308824</td>\n",
       "      <td>0.654412</td>\n",
       "      <td>0.242647</td>\n",
       "      <td>0.985294</td>\n",
       "      <td>0.963235</td>\n",
       "      <td>0.169118</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.286765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.602941</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.566176</td>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.933824</td>\n",
       "      <td>0.838235</td>\n",
       "      <td>0.838235</td>\n",
       "      <td>0.963235</td>\n",
       "      <td>0.963235</td>\n",
       "      <td>0.301471</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.257353</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.367647</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.933824</td>\n",
       "      <td>0.323529</td>\n",
       "      <td>0.154412</td>\n",
       "      <td>0.154412</td>\n",
       "      <td>0.389706</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.595588</td>\n",
       "      <td>0.161765</td>\n",
       "      <td>0.772059</td>\n",
       "      <td>0.227941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.286765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.257353</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.536765</td>\n",
       "      <td>0.272059</td>\n",
       "      <td>0.610294</td>\n",
       "      <td>0.139706</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.154412</td>\n",
       "      <td>0.691176</td>\n",
       "      <td>0.830882</td>\n",
       "      <td>0.713235</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.338235</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.286765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.955882</td>\n",
       "      <td>0.955882</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.713235</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.272059</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.330882</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.139706</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.154412</td>\n",
       "      <td>0.720588</td>\n",
       "      <td>0.830882</td>\n",
       "      <td>0.889706</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.272059</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.286765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.955882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.330882</td>\n",
       "      <td>0.073529</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.919118</td>\n",
       "      <td>0.301471</td>\n",
       "      <td>0.154412</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.933824</td>\n",
       "      <td>0.227941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.286765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        edad  genero       ANB    ICS/SN      IMPA  ICI/PLANO  interincisal  \\\n",
       "0   0.441176     0.0  0.455882  0.985294  0.014706   0.227941      0.213235   \n",
       "1   1.000000     1.0  0.198529  0.750000  0.110294   0.852941      0.117647   \n",
       "2   0.022059     1.0  0.198529  0.176471  0.558824   0.051471      0.941176   \n",
       "3   0.257353     1.0  0.757353  0.110294  0.735294   0.073529      0.705882   \n",
       "4   0.602941     1.0  0.625000  0.750000  0.823529   0.566176      0.382353   \n",
       "..       ...     ...       ...       ...       ...        ...           ...   \n",
       "64  0.257353     1.0  0.455882  0.367647  0.735294   0.933824      0.323529   \n",
       "65  0.257353     1.0  0.536765  0.272059  0.610294   0.139706      0.764706   \n",
       "66  0.441176     0.0  0.625000  0.911765  0.000000   0.794118      0.000000   \n",
       "67  0.125000     1.0  0.625000  0.330882  0.558824   0.139706      0.794118   \n",
       "68  0.955882     0.0  0.330882  0.073529  0.970588   0.647059      0.919118   \n",
       "\n",
       "    lab sup/lin E  lab inf/lin E  nasolabial   Overjet  Oberbite       j-s  \\\n",
       "0        0.713235       0.720588    0.720588  0.933824  0.411765  0.654412   \n",
       "1        0.301471       0.661765    0.066176  0.080882  0.411765  0.595588   \n",
       "2        0.301471       0.661765    0.088235  0.500000  0.411765  0.419118   \n",
       "3        0.713235       0.308824    0.654412  0.242647  0.985294  0.963235   \n",
       "4        0.933824       0.838235    0.838235  0.963235  0.963235  0.301471   \n",
       "..            ...            ...         ...       ...       ...       ...   \n",
       "64       0.154412       0.154412    0.389706  0.500000  0.595588  0.161765   \n",
       "65       0.558824       0.154412    0.691176  0.830882  0.713235  0.514706   \n",
       "66       0.955882       0.955882    0.250000  0.500000  0.713235  0.794118   \n",
       "67       0.558824       0.154412    0.720588  0.830882  0.889706  0.514706   \n",
       "68       0.301471       0.154412    0.514706  0.500000  0.411765  0.205882   \n",
       "\n",
       "      Angulo  Discrepancia  Clase    Perfil  \n",
       "0   0.713235      1.000000    1.0  0.661765  \n",
       "1   0.816176      0.000000    0.0  0.661765  \n",
       "2   0.573529      1.000000    0.0  0.661765  \n",
       "3   0.169118      1.000000    0.0  0.286765  \n",
       "4   0.500000      0.485294    0.0  0.875000  \n",
       "..       ...           ...    ...       ...  \n",
       "64  0.772059      0.227941    0.0  0.286765  \n",
       "65  0.338235      1.000000    1.0  0.286765  \n",
       "66  0.272059      1.000000    1.0  1.000000  \n",
       "67  0.272059      1.000000    1.0  0.286765  \n",
       "68  0.933824      0.227941    0.0  0.286765  \n",
       "\n",
       "[69 rows x 17 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "# transform = prp.MinMaxScaler()#82\n",
    "# transform = prp.RobustScaler()\n",
    "# transform = prp.StandardScaler()\n",
    "transform = prp.QuantileTransformer(output_distribution='uniform')\n",
    "colums = transform.fit_transform(df_test_enc)\n",
    "df_test_enc_est = pd.DataFrame(colums)\n",
    "df_test_enc_est.columns = df_test_enc.columns\n",
    "df_test_enc_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 7051,
     "status": "ok",
     "timestamp": 1693252366280,
     "user": {
      "displayName": "ShadowMan",
      "userId": "06933040393855637082"
     },
     "user_tz": 420
    },
    "id": "yWqdIg_XvjSh",
    "outputId": "3bd4d36f-ac3a-4bfc-e7d4-786eec9ed4c9"
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(20,30))\n",
    "i = 1\n",
    "for column in df_train_enc_est:\n",
    "  plt.subplot(6,3,i)\n",
    "  plt.violinplot(dataset=df_train_enc_est[column],vert=False)\n",
    "  plt.title(column)\n",
    "  i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3VAYtS1JhY9"
   },
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Feature Selection with Shap Values and TensorFlow\n",
    "\n",
    "# model_select = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Dense(64, input_shape=(df_train_enc_est.shape[1],), activation ='elu'),\n",
    "#     tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "#     tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "# ])\n",
    "\n",
    "# model_select.compile(\n",
    "#     optimizer = tf.keras.optimizers.Adam(),\n",
    "#     loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "#     metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "# )\n",
    "\n",
    "# model_select.fit(df_train_enc_est, y_train, validation_data = (df_test_enc_est, y_test), epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explainer = shap.GradientExplainer(model_select, df_train_enc_est.values)\n",
    "# shap_values = explainer.shap_values(df_test_enc_est)\n",
    "\n",
    "# shap.summary_plot(shap_values, df_test_enc_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection with Shap Values and TensorFlow\n",
    "\n",
    "model_select = MLPClassifier(\n",
    "    hidden_layer_sizes=(20,),activation='logistic', max_iter=1000,learning_rate='invscaling',verbose = True, random_state=0)\n",
    "\n",
    "\n",
    "model_select.fit(df_train_enc_est, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(model_select.predict,df_train_enc_est)\n",
    "shap_values = explainer.shap_values(df_test_enc_est)\n",
    "\n",
    "shap.summary_plot(shap_values, df_test_enc_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1693252366280,
     "user": {
      "displayName": "ShadowMan",
      "userId": "06933040393855637082"
     },
     "user_tz": 420
    },
    "id": "VcdOgdKTJmZe",
    "outputId": "b42c0ec3-ab82-44d9-f644-15b61055de44"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "bestfeatures = SelectKBest(k='all')\n",
    "fit = bestfeatures.fit(df_train_enc_est, y_train)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(df_train_enc_est.columns)\n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']\n",
    "print(featureScores.nlargest(17,'Score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZCjMya5aYfam"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMi0f0SvM1_F"
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import SelectKBest\n",
    "# #apply SelectKBest class to show the highest features' scores\n",
    "# bestfeatures = SelectKBest(k='all')\n",
    "# fit = bestfeatures.fit(df_X_train_stand, y_train)\n",
    "# dfscores = pd.DataFrame(fit.scores_)\n",
    "# dfcolumns = pd.DataFrame(df_X_train_stand.columns)\n",
    "# #concat two dataframes for better visualization\n",
    "# featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "# featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "# print(featureScores.nlargest(17,'Score'))  #sort best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1693252366282,
     "user": {
      "displayName": "ShadowMan",
      "userId": "06933040393855637082"
     },
     "user_tz": 420
    },
    "id": "X69SDb6eNGf9",
    "outputId": "a96537f4-68f2-49c4-b0aa-f1c5a58212c4"
   },
   "outputs": [],
   "source": [
    "df_train_enc_est.drop(['genero','ICI/PLANO','Oberbite','nasolabial','ICS/SN','IMPA'],axis=1)\n",
    "df_test_enc_est.drop(['genero','ICI/PLANO','Oberbite','nasolabial','ICS/SN','IMPA'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_enc_est.drop(['Perfil','lab sup/lin E','Oberbite','ANB','lab inf/lin E',],axis=1)\n",
    "df_test_enc_est.drop(['Perfil','lab sup/lin E','Oberbite','ANB','lab inf/lin E',],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUKUzxE67q2s"
   },
   "source": [
    "## Entrenamiento de modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4e8W7edzRH6j",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "# Define el modelo dentro de una función que tomará los hiperparámetros como argumentos\n",
    "def create_model(activation_1, activation_2, activation_3, activation_4, activation_5, activation_6, \n",
    "                 init_mode_1, init_mode_2, init_mode_3, init_mode_4, init_mode_5, init_mode_6, regularizer_type):\n",
    "    \n",
    "    if regularizer_type == \"l1\":\n",
    "        regularizer = keras.regularizers.l1(0.01)\n",
    "    elif regularizer_type == \"l2\":\n",
    "        regularizer = keras.regularizers.l2(0.01)\n",
    "    else:\n",
    "        regularizer = None\n",
    "        \n",
    "        \n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(4, input_shape=(df_train_enc_est.shape[1],), activation=activation_1, \n",
    "                           kernel_initializer=init_mode_1, kernel_regularizer=regularizer_type),\n",
    "        keras.layers.Dense(4, activation=activation_2, kernel_initializer=init_mode_2),\n",
    "        keras.layers.Dense(4, activation=activation_3, kernel_initializer=init_mode_3),\n",
    "        keras.layers.Dense(4, activation=activation_4, kernel_initializer=init_mode_4),\n",
    "        keras.layers.Dense(4, activation=activation_5, kernel_initializer=init_mode_5),\n",
    "        keras.layers.Dense(1, activation=activation_6, kernel_initializer=init_mode_6),\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer = tf.keras.optimizers.Adam(),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "    return model\n",
    "\n",
    "# Función objetivo para Optuna\n",
    "def objective(trial):\n",
    "    \n",
    "    regularizer_type = trial.suggest_categorical('regularizer_type', [None, \"l1\", \"l2\"])\n",
    "    \n",
    "    activations_list = [\"elu\",\"exponential\",\"hard_sigmoid\",\"linear\", \"relu\",\"selu\",\"sigmoid\",\"softmax\",\"softplus\",\n",
    "                                                                                         \"softsign\",\"swish\",\"tanh\"]\n",
    "    activation_1 = trial.suggest_categorical('activation_1', activations_list)\n",
    "    activation_2 = trial.suggest_categorical('activation_2', activations_list)\n",
    "    activation_3 = trial.suggest_categorical('activation_3', activations_list)\n",
    "    activation_4 = trial.suggest_categorical('activation_4', activations_list)\n",
    "    activation_5 = trial.suggest_categorical('activation_5', activations_list)\n",
    "    activation_6 = trial.suggest_categorical('activation_6', activations_list)\n",
    "    \n",
    "    init_list = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "    init_mode_1 = trial.suggest_categorical('init_mode_1', init_list)\n",
    "    init_mode_2 = trial.suggest_categorical('init_mode_2', init_list)\n",
    "    init_mode_3 = trial.suggest_categorical('init_mode_3', init_list)\n",
    "    init_mode_4 = trial.suggest_categorical('init_mode_4', init_list)\n",
    "    init_mode_5 = trial.suggest_categorical('init_mode_5', init_list)\n",
    "    init_mode_6 = trial.suggest_categorical('init_mode_6', init_list)\n",
    "              \n",
    "    model = create_model(activation_1, activation_2, activation_3, activation_4, activation_5, activation_6,\n",
    "                 init_mode_1, init_mode_2, init_mode_3, init_mode_4, init_mode_5, init_mode_6, regularizer_type)\n",
    "    history = model.fit(df_train_enc_est, y_train, epochs=50, validation_split=0.2, verbose=0)\n",
    "    \n",
    "    # Usa la pérdida de validación como métrica para optimizar\n",
    "    val_loss = history.history['binary_accuracy'][-1]\n",
    "    return val_loss\n",
    "\n",
    "# Iniciar la optimización con Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(\"Mejores hiperparámetros:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HYKjug8ARIB7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "9/9 [==============================] - 2s 42ms/step - loss: 0.7042 - binary_accuracy: 0.4853 - val_loss: 0.6875 - val_binary_accuracy: 0.5507\n",
      "Epoch 2/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.7027 - binary_accuracy: 0.4853 - val_loss: 0.6872 - val_binary_accuracy: 0.5507\n",
      "Epoch 3/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.7012 - binary_accuracy: 0.4853 - val_loss: 0.6869 - val_binary_accuracy: 0.5507\n",
      "Epoch 4/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.6998 - binary_accuracy: 0.4853 - val_loss: 0.6866 - val_binary_accuracy: 0.5507\n",
      "Epoch 5/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6990 - binary_accuracy: 0.4853 - val_loss: 0.6862 - val_binary_accuracy: 0.5507\n",
      "Epoch 6/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.6976 - binary_accuracy: 0.4853 - val_loss: 0.6858 - val_binary_accuracy: 0.5507\n",
      "Epoch 7/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6963 - binary_accuracy: 0.4853 - val_loss: 0.6851 - val_binary_accuracy: 0.5507\n",
      "Epoch 8/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.6950 - binary_accuracy: 0.4853 - val_loss: 0.6842 - val_binary_accuracy: 0.5507\n",
      "Epoch 9/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.6937 - binary_accuracy: 0.4853 - val_loss: 0.6830 - val_binary_accuracy: 0.5507\n",
      "Epoch 10/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.6920 - binary_accuracy: 0.4853 - val_loss: 0.6808 - val_binary_accuracy: 0.5507\n",
      "Epoch 11/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6900 - binary_accuracy: 0.4853 - val_loss: 0.6785 - val_binary_accuracy: 0.5507\n",
      "Epoch 12/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6872 - binary_accuracy: 0.4853 - val_loss: 0.6748 - val_binary_accuracy: 0.5507\n",
      "Epoch 13/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6841 - binary_accuracy: 0.4853 - val_loss: 0.6698 - val_binary_accuracy: 0.5507\n",
      "Epoch 14/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6802 - binary_accuracy: 0.4853 - val_loss: 0.6638 - val_binary_accuracy: 0.5507\n",
      "Epoch 15/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.6751 - binary_accuracy: 0.4853 - val_loss: 0.6560 - val_binary_accuracy: 0.5507\n",
      "Epoch 16/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6691 - binary_accuracy: 0.5551 - val_loss: 0.6473 - val_binary_accuracy: 0.6232\n",
      "Epoch 17/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6623 - binary_accuracy: 0.6360 - val_loss: 0.6362 - val_binary_accuracy: 0.6667\n",
      "Epoch 18/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6549 - binary_accuracy: 0.6801 - val_loss: 0.6237 - val_binary_accuracy: 0.7246\n",
      "Epoch 19/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6474 - binary_accuracy: 0.6985 - val_loss: 0.6098 - val_binary_accuracy: 0.7536\n",
      "Epoch 20/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6384 - binary_accuracy: 0.7316 - val_loss: 0.5972 - val_binary_accuracy: 0.7826\n",
      "Epoch 21/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6304 - binary_accuracy: 0.7390 - val_loss: 0.5844 - val_binary_accuracy: 0.7826\n",
      "Epoch 22/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6221 - binary_accuracy: 0.7647 - val_loss: 0.5741 - val_binary_accuracy: 0.8406\n",
      "Epoch 23/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.6146 - binary_accuracy: 0.7647 - val_loss: 0.5631 - val_binary_accuracy: 0.8551\n",
      "Epoch 24/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6073 - binary_accuracy: 0.7721 - val_loss: 0.5533 - val_binary_accuracy: 0.8551\n",
      "Epoch 25/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6004 - binary_accuracy: 0.7684 - val_loss: 0.5434 - val_binary_accuracy: 0.8551\n",
      "Epoch 26/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5939 - binary_accuracy: 0.7831 - val_loss: 0.5366 - val_binary_accuracy: 0.8551\n",
      "Epoch 27/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5876 - binary_accuracy: 0.7941 - val_loss: 0.5293 - val_binary_accuracy: 0.8551\n",
      "Epoch 28/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5822 - binary_accuracy: 0.7978 - val_loss: 0.5207 - val_binary_accuracy: 0.8696\n",
      "Epoch 29/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5762 - binary_accuracy: 0.8088 - val_loss: 0.5155 - val_binary_accuracy: 0.8551\n",
      "Epoch 30/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5713 - binary_accuracy: 0.8015 - val_loss: 0.5093 - val_binary_accuracy: 0.8551\n",
      "Epoch 31/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5661 - binary_accuracy: 0.8088 - val_loss: 0.5040 - val_binary_accuracy: 0.8551\n",
      "Epoch 32/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5620 - binary_accuracy: 0.8125 - val_loss: 0.5019 - val_binary_accuracy: 0.8551\n",
      "Epoch 33/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5568 - binary_accuracy: 0.8162 - val_loss: 0.4945 - val_binary_accuracy: 0.8551\n",
      "Epoch 34/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.5522 - binary_accuracy: 0.8162 - val_loss: 0.4896 - val_binary_accuracy: 0.8551\n",
      "Epoch 35/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.5487 - binary_accuracy: 0.8162 - val_loss: 0.4853 - val_binary_accuracy: 0.8551\n",
      "Epoch 36/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5444 - binary_accuracy: 0.8162 - val_loss: 0.4792 - val_binary_accuracy: 0.8551\n",
      "Epoch 37/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5412 - binary_accuracy: 0.8125 - val_loss: 0.4755 - val_binary_accuracy: 0.8551\n",
      "Epoch 38/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.5370 - binary_accuracy: 0.8088 - val_loss: 0.4726 - val_binary_accuracy: 0.8696\n",
      "Epoch 39/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5340 - binary_accuracy: 0.8235 - val_loss: 0.4709 - val_binary_accuracy: 0.8696\n",
      "Epoch 40/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5312 - binary_accuracy: 0.8235 - val_loss: 0.4693 - val_binary_accuracy: 0.8696\n",
      "Epoch 41/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5274 - binary_accuracy: 0.8235 - val_loss: 0.4632 - val_binary_accuracy: 0.8696\n",
      "Epoch 42/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5245 - binary_accuracy: 0.8162 - val_loss: 0.4568 - val_binary_accuracy: 0.8696\n",
      "Epoch 43/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5209 - binary_accuracy: 0.8199 - val_loss: 0.4546 - val_binary_accuracy: 0.8696\n",
      "Epoch 44/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.5181 - binary_accuracy: 0.8199 - val_loss: 0.4522 - val_binary_accuracy: 0.8696\n",
      "Epoch 45/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.5148 - binary_accuracy: 0.8199 - val_loss: 0.4470 - val_binary_accuracy: 0.8696\n",
      "Epoch 46/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.5122 - binary_accuracy: 0.8199 - val_loss: 0.4428 - val_binary_accuracy: 0.8696\n",
      "Epoch 47/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5095 - binary_accuracy: 0.8199 - val_loss: 0.4399 - val_binary_accuracy: 0.8696\n",
      "Epoch 48/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.5071 - binary_accuracy: 0.8199 - val_loss: 0.4363 - val_binary_accuracy: 0.8696\n",
      "Epoch 49/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.5041 - binary_accuracy: 0.8199 - val_loss: 0.4340 - val_binary_accuracy: 0.8696\n",
      "Epoch 50/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5024 - binary_accuracy: 0.8162 - val_loss: 0.4356 - val_binary_accuracy: 0.8841\n",
      "Epoch 51/500\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.5013 - binary_accuracy: 0.8162 - val_loss: 0.4263 - val_binary_accuracy: 0.8696\n",
      "Epoch 52/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4961 - binary_accuracy: 0.8199 - val_loss: 0.4279 - val_binary_accuracy: 0.8841\n",
      "Epoch 53/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4936 - binary_accuracy: 0.8199 - val_loss: 0.4224 - val_binary_accuracy: 0.8841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4910 - binary_accuracy: 0.8199 - val_loss: 0.4196 - val_binary_accuracy: 0.8696\n",
      "Epoch 55/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4898 - binary_accuracy: 0.8162 - val_loss: 0.4116 - val_binary_accuracy: 0.8986\n",
      "Epoch 56/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4859 - binary_accuracy: 0.8199 - val_loss: 0.4154 - val_binary_accuracy: 0.8696\n",
      "Epoch 57/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4843 - binary_accuracy: 0.8235 - val_loss: 0.4116 - val_binary_accuracy: 0.8841\n",
      "Epoch 58/500\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 0.4814 - binary_accuracy: 0.8235 - val_loss: 0.4090 - val_binary_accuracy: 0.8986\n",
      "Epoch 59/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.4790 - binary_accuracy: 0.8309 - val_loss: 0.4098 - val_binary_accuracy: 0.8841\n",
      "Epoch 60/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4767 - binary_accuracy: 0.8309 - val_loss: 0.4056 - val_binary_accuracy: 0.8986\n",
      "Epoch 61/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4753 - binary_accuracy: 0.8309 - val_loss: 0.3928 - val_binary_accuracy: 0.8986\n",
      "Epoch 62/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4724 - binary_accuracy: 0.8309 - val_loss: 0.3912 - val_binary_accuracy: 0.8986\n",
      "Epoch 63/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4711 - binary_accuracy: 0.8346 - val_loss: 0.3988 - val_binary_accuracy: 0.8841\n",
      "Epoch 64/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4681 - binary_accuracy: 0.8309 - val_loss: 0.3937 - val_binary_accuracy: 0.8841\n",
      "Epoch 65/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4654 - binary_accuracy: 0.8309 - val_loss: 0.3821 - val_binary_accuracy: 0.8986\n",
      "Epoch 66/500\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.4631 - binary_accuracy: 0.8309 - val_loss: 0.3795 - val_binary_accuracy: 0.9130\n",
      "Epoch 67/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4613 - binary_accuracy: 0.8309 - val_loss: 0.3806 - val_binary_accuracy: 0.8841\n",
      "Epoch 68/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4584 - binary_accuracy: 0.8346 - val_loss: 0.3757 - val_binary_accuracy: 0.9130\n",
      "Epoch 69/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4580 - binary_accuracy: 0.8309 - val_loss: 0.3814 - val_binary_accuracy: 0.8841\n",
      "Epoch 70/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4543 - binary_accuracy: 0.8382 - val_loss: 0.3705 - val_binary_accuracy: 0.8986\n",
      "Epoch 71/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4593 - binary_accuracy: 0.8235 - val_loss: 0.3586 - val_binary_accuracy: 0.8986\n",
      "Epoch 72/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4491 - binary_accuracy: 0.8309 - val_loss: 0.3667 - val_binary_accuracy: 0.8986\n",
      "Epoch 73/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4495 - binary_accuracy: 0.8382 - val_loss: 0.3686 - val_binary_accuracy: 0.8986\n",
      "Epoch 74/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4466 - binary_accuracy: 0.8346 - val_loss: 0.3559 - val_binary_accuracy: 0.8986\n",
      "Epoch 75/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4435 - binary_accuracy: 0.8346 - val_loss: 0.3548 - val_binary_accuracy: 0.8841\n",
      "Epoch 76/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4404 - binary_accuracy: 0.8382 - val_loss: 0.3508 - val_binary_accuracy: 0.8841\n",
      "Epoch 77/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4382 - binary_accuracy: 0.8419 - val_loss: 0.3496 - val_binary_accuracy: 0.8841\n",
      "Epoch 78/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4368 - binary_accuracy: 0.8382 - val_loss: 0.3518 - val_binary_accuracy: 0.9130\n",
      "Epoch 79/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4335 - binary_accuracy: 0.8493 - val_loss: 0.3456 - val_binary_accuracy: 0.8986\n",
      "Epoch 80/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4319 - binary_accuracy: 0.8419 - val_loss: 0.3392 - val_binary_accuracy: 0.8841\n",
      "Epoch 81/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4292 - binary_accuracy: 0.8493 - val_loss: 0.3398 - val_binary_accuracy: 0.8986\n",
      "Epoch 82/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4270 - binary_accuracy: 0.8529 - val_loss: 0.3367 - val_binary_accuracy: 0.8986\n",
      "Epoch 83/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4250 - binary_accuracy: 0.8456 - val_loss: 0.3324 - val_binary_accuracy: 0.8841\n",
      "Epoch 84/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4233 - binary_accuracy: 0.8529 - val_loss: 0.3306 - val_binary_accuracy: 0.8986\n",
      "Epoch 85/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4219 - binary_accuracy: 0.8493 - val_loss: 0.3306 - val_binary_accuracy: 0.9130\n",
      "Epoch 86/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4196 - binary_accuracy: 0.8566 - val_loss: 0.3284 - val_binary_accuracy: 0.9130\n",
      "Epoch 87/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4177 - binary_accuracy: 0.8603 - val_loss: 0.3248 - val_binary_accuracy: 0.9130\n",
      "Epoch 88/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4165 - binary_accuracy: 0.8566 - val_loss: 0.3219 - val_binary_accuracy: 0.8986\n",
      "Epoch 89/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4144 - binary_accuracy: 0.8676 - val_loss: 0.3214 - val_binary_accuracy: 0.9130\n",
      "Epoch 90/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4150 - binary_accuracy: 0.8640 - val_loss: 0.3242 - val_binary_accuracy: 0.9130\n",
      "Epoch 91/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4135 - binary_accuracy: 0.8640 - val_loss: 0.3152 - val_binary_accuracy: 0.9130\n",
      "Epoch 92/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4102 - binary_accuracy: 0.8713 - val_loss: 0.3174 - val_binary_accuracy: 0.9130\n",
      "Epoch 93/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4094 - binary_accuracy: 0.8713 - val_loss: 0.3178 - val_binary_accuracy: 0.9130\n",
      "Epoch 94/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4063 - binary_accuracy: 0.8750 - val_loss: 0.3113 - val_binary_accuracy: 0.9130\n",
      "Epoch 95/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4091 - binary_accuracy: 0.8640 - val_loss: 0.3076 - val_binary_accuracy: 0.9130\n",
      "Epoch 96/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4052 - binary_accuracy: 0.8676 - val_loss: 0.3136 - val_binary_accuracy: 0.9130\n",
      "Epoch 97/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4037 - binary_accuracy: 0.8750 - val_loss: 0.3059 - val_binary_accuracy: 0.9130\n",
      "Epoch 98/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4021 - binary_accuracy: 0.8750 - val_loss: 0.3058 - val_binary_accuracy: 0.9130\n",
      "Epoch 99/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4013 - binary_accuracy: 0.8750 - val_loss: 0.3042 - val_binary_accuracy: 0.9130\n",
      "Epoch 100/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3999 - binary_accuracy: 0.8750 - val_loss: 0.3041 - val_binary_accuracy: 0.9130\n",
      "Epoch 101/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3984 - binary_accuracy: 0.8750 - val_loss: 0.3031 - val_binary_accuracy: 0.9130\n",
      "Epoch 102/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3980 - binary_accuracy: 0.8713 - val_loss: 0.3040 - val_binary_accuracy: 0.9130\n",
      "Epoch 103/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3974 - binary_accuracy: 0.8750 - val_loss: 0.2982 - val_binary_accuracy: 0.9130\n",
      "Epoch 104/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3955 - binary_accuracy: 0.8750 - val_loss: 0.3010 - val_binary_accuracy: 0.9130\n",
      "Epoch 105/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3947 - binary_accuracy: 0.8750 - val_loss: 0.2996 - val_binary_accuracy: 0.9130\n",
      "Epoch 106/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3935 - binary_accuracy: 0.8750 - val_loss: 0.2950 - val_binary_accuracy: 0.9130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3932 - binary_accuracy: 0.8787 - val_loss: 0.2945 - val_binary_accuracy: 0.9130\n",
      "Epoch 108/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3962 - binary_accuracy: 0.8713 - val_loss: 0.3024 - val_binary_accuracy: 0.9130\n",
      "Epoch 109/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3906 - binary_accuracy: 0.8713 - val_loss: 0.2884 - val_binary_accuracy: 0.9130\n",
      "Epoch 110/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3931 - binary_accuracy: 0.8787 - val_loss: 0.2872 - val_binary_accuracy: 0.9130\n",
      "Epoch 111/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3921 - binary_accuracy: 0.8713 - val_loss: 0.3100 - val_binary_accuracy: 0.9130\n",
      "Epoch 112/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3913 - binary_accuracy: 0.8787 - val_loss: 0.2882 - val_binary_accuracy: 0.9130\n",
      "Epoch 113/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3887 - binary_accuracy: 0.8750 - val_loss: 0.2913 - val_binary_accuracy: 0.9130\n",
      "Epoch 114/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3887 - binary_accuracy: 0.8750 - val_loss: 0.2841 - val_binary_accuracy: 0.9130\n",
      "Epoch 115/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3863 - binary_accuracy: 0.8824 - val_loss: 0.2946 - val_binary_accuracy: 0.9130\n",
      "Epoch 116/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3874 - binary_accuracy: 0.8787 - val_loss: 0.2918 - val_binary_accuracy: 0.9130\n",
      "Epoch 117/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3848 - binary_accuracy: 0.8824 - val_loss: 0.2836 - val_binary_accuracy: 0.9130\n",
      "Epoch 118/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3862 - binary_accuracy: 0.8750 - val_loss: 0.2922 - val_binary_accuracy: 0.9130\n",
      "Epoch 119/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3836 - binary_accuracy: 0.8787 - val_loss: 0.2856 - val_binary_accuracy: 0.9130\n",
      "Epoch 120/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3845 - binary_accuracy: 0.8787 - val_loss: 0.2851 - val_binary_accuracy: 0.9130\n",
      "Epoch 121/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3841 - binary_accuracy: 0.8787 - val_loss: 0.2889 - val_binary_accuracy: 0.9130\n",
      "Epoch 122/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3822 - binary_accuracy: 0.8787 - val_loss: 0.2852 - val_binary_accuracy: 0.9130\n",
      "Epoch 123/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3816 - binary_accuracy: 0.8787 - val_loss: 0.2852 - val_binary_accuracy: 0.9130\n",
      "Epoch 124/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3810 - binary_accuracy: 0.8787 - val_loss: 0.2857 - val_binary_accuracy: 0.9130\n",
      "Epoch 125/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3816 - binary_accuracy: 0.8750 - val_loss: 0.2822 - val_binary_accuracy: 0.9130\n",
      "Epoch 126/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3802 - binary_accuracy: 0.8787 - val_loss: 0.2885 - val_binary_accuracy: 0.9130\n",
      "Epoch 127/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3798 - binary_accuracy: 0.8787 - val_loss: 0.2823 - val_binary_accuracy: 0.9130\n",
      "Epoch 128/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3789 - binary_accuracy: 0.8787 - val_loss: 0.2836 - val_binary_accuracy: 0.9130\n",
      "Epoch 129/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3784 - binary_accuracy: 0.8787 - val_loss: 0.2835 - val_binary_accuracy: 0.9130\n",
      "Epoch 130/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3778 - binary_accuracy: 0.8750 - val_loss: 0.2799 - val_binary_accuracy: 0.9130\n",
      "Epoch 131/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3774 - binary_accuracy: 0.8787 - val_loss: 0.2844 - val_binary_accuracy: 0.9130\n",
      "Epoch 132/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3774 - binary_accuracy: 0.8787 - val_loss: 0.2791 - val_binary_accuracy: 0.9130\n",
      "Epoch 133/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3769 - binary_accuracy: 0.8787 - val_loss: 0.2892 - val_binary_accuracy: 0.9130\n",
      "Epoch 134/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3800 - binary_accuracy: 0.8713 - val_loss: 0.2797 - val_binary_accuracy: 0.9130\n",
      "Epoch 135/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3745 - binary_accuracy: 0.8787 - val_loss: 0.2887 - val_binary_accuracy: 0.9130\n",
      "Epoch 136/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3766 - binary_accuracy: 0.8787 - val_loss: 0.2794 - val_binary_accuracy: 0.9130\n",
      "Epoch 137/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3741 - binary_accuracy: 0.8787 - val_loss: 0.2875 - val_binary_accuracy: 0.9130\n",
      "Epoch 138/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3725 - binary_accuracy: 0.8787 - val_loss: 0.2741 - val_binary_accuracy: 0.9130\n",
      "Epoch 139/500\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.3745 - binary_accuracy: 0.8750 - val_loss: 0.2753 - val_binary_accuracy: 0.9130\n",
      "Epoch 140/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3721 - binary_accuracy: 0.8787 - val_loss: 0.2870 - val_binary_accuracy: 0.9130\n",
      "Epoch 141/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3773 - binary_accuracy: 0.8640 - val_loss: 0.2750 - val_binary_accuracy: 0.9130\n",
      "Epoch 142/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3714 - binary_accuracy: 0.8787 - val_loss: 0.2922 - val_binary_accuracy: 0.9130\n",
      "Epoch 143/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3718 - binary_accuracy: 0.8713 - val_loss: 0.2783 - val_binary_accuracy: 0.9130\n",
      "Epoch 144/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3694 - binary_accuracy: 0.8750 - val_loss: 0.2799 - val_binary_accuracy: 0.9130\n",
      "Epoch 145/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3704 - binary_accuracy: 0.8787 - val_loss: 0.2844 - val_binary_accuracy: 0.9130\n",
      "Epoch 146/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3695 - binary_accuracy: 0.8713 - val_loss: 0.2762 - val_binary_accuracy: 0.9130\n",
      "Epoch 147/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3693 - binary_accuracy: 0.8640 - val_loss: 0.2831 - val_binary_accuracy: 0.9130\n",
      "Epoch 148/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3678 - binary_accuracy: 0.8750 - val_loss: 0.2796 - val_binary_accuracy: 0.9130\n",
      "Epoch 149/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3667 - binary_accuracy: 0.8750 - val_loss: 0.2818 - val_binary_accuracy: 0.9130\n",
      "Epoch 150/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3667 - binary_accuracy: 0.8787 - val_loss: 0.2746 - val_binary_accuracy: 0.9130\n",
      "Epoch 151/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3662 - binary_accuracy: 0.8750 - val_loss: 0.2841 - val_binary_accuracy: 0.9130\n",
      "Epoch 152/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3645 - binary_accuracy: 0.8824 - val_loss: 0.2773 - val_binary_accuracy: 0.9130\n",
      "Epoch 153/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3650 - binary_accuracy: 0.8824 - val_loss: 0.2804 - val_binary_accuracy: 0.9130\n",
      "Epoch 154/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3635 - binary_accuracy: 0.8824 - val_loss: 0.2761 - val_binary_accuracy: 0.9130\n",
      "Epoch 155/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3637 - binary_accuracy: 0.8824 - val_loss: 0.2771 - val_binary_accuracy: 0.9130\n",
      "Epoch 156/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3631 - binary_accuracy: 0.8824 - val_loss: 0.2788 - val_binary_accuracy: 0.9130\n",
      "Epoch 157/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3650 - binary_accuracy: 0.8787 - val_loss: 0.2814 - val_binary_accuracy: 0.9130\n",
      "Epoch 158/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3646 - binary_accuracy: 0.8787 - val_loss: 0.2746 - val_binary_accuracy: 0.9130\n",
      "Epoch 159/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3617 - binary_accuracy: 0.8824 - val_loss: 0.2884 - val_binary_accuracy: 0.9130\n",
      "Epoch 160/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3630 - binary_accuracy: 0.8750 - val_loss: 0.2812 - val_binary_accuracy: 0.9130\n",
      "Epoch 161/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3625 - binary_accuracy: 0.8750 - val_loss: 0.2758 - val_binary_accuracy: 0.9130\n",
      "Epoch 162/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3612 - binary_accuracy: 0.8787 - val_loss: 0.2882 - val_binary_accuracy: 0.9130\n",
      "Epoch 163/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3610 - binary_accuracy: 0.8860 - val_loss: 0.2803 - val_binary_accuracy: 0.9130\n",
      "Epoch 164/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3603 - binary_accuracy: 0.8787 - val_loss: 0.2758 - val_binary_accuracy: 0.9130\n",
      "Epoch 165/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3599 - binary_accuracy: 0.8787 - val_loss: 0.2829 - val_binary_accuracy: 0.9130\n",
      "Epoch 166/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3604 - binary_accuracy: 0.8824 - val_loss: 0.2822 - val_binary_accuracy: 0.9130\n",
      "Epoch 167/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3599 - binary_accuracy: 0.8860 - val_loss: 0.2771 - val_binary_accuracy: 0.9130\n",
      "Epoch 168/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3600 - binary_accuracy: 0.8824 - val_loss: 0.2805 - val_binary_accuracy: 0.9130\n",
      "Epoch 169/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3596 - binary_accuracy: 0.8824 - val_loss: 0.2824 - val_binary_accuracy: 0.9130\n",
      "Epoch 170/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3581 - binary_accuracy: 0.8860 - val_loss: 0.2738 - val_binary_accuracy: 0.9130\n",
      "Epoch 171/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3604 - binary_accuracy: 0.8750 - val_loss: 0.2771 - val_binary_accuracy: 0.9130\n",
      "Epoch 172/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3585 - binary_accuracy: 0.8860 - val_loss: 0.2819 - val_binary_accuracy: 0.9130\n",
      "Epoch 173/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3594 - binary_accuracy: 0.8750 - val_loss: 0.2745 - val_binary_accuracy: 0.9130\n",
      "Epoch 174/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3589 - binary_accuracy: 0.8824 - val_loss: 0.2841 - val_binary_accuracy: 0.9130\n",
      "Epoch 175/500\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.3586 - binary_accuracy: 0.8824 - val_loss: 0.2757 - val_binary_accuracy: 0.9130\n",
      "Epoch 176/500\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.3606 - binary_accuracy: 0.8824 - val_loss: 0.2823 - val_binary_accuracy: 0.9130\n",
      "Epoch 177/500\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.3618 - binary_accuracy: 0.8750 - val_loss: 0.2747 - val_binary_accuracy: 0.9130\n",
      "Epoch 178/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3566 - binary_accuracy: 0.8824 - val_loss: 0.2850 - val_binary_accuracy: 0.9130\n",
      "Epoch 179/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3577 - binary_accuracy: 0.8860 - val_loss: 0.2746 - val_binary_accuracy: 0.9130\n",
      "Epoch 180/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3568 - binary_accuracy: 0.8824 - val_loss: 0.2782 - val_binary_accuracy: 0.9130\n",
      "Epoch 181/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3573 - binary_accuracy: 0.8860 - val_loss: 0.2801 - val_binary_accuracy: 0.9130\n",
      "Epoch 182/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3581 - binary_accuracy: 0.8824 - val_loss: 0.2764 - val_binary_accuracy: 0.9130\n",
      "Epoch 183/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3566 - binary_accuracy: 0.8860 - val_loss: 0.2772 - val_binary_accuracy: 0.9130\n",
      "Epoch 184/500\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.3565 - binary_accuracy: 0.8860 - val_loss: 0.2736 - val_binary_accuracy: 0.9130\n",
      "Epoch 185/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3571 - binary_accuracy: 0.8860 - val_loss: 0.2783 - val_binary_accuracy: 0.9130\n",
      "Epoch 186/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3564 - binary_accuracy: 0.8824 - val_loss: 0.2781 - val_binary_accuracy: 0.9130\n",
      "Epoch 187/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3561 - binary_accuracy: 0.8824 - val_loss: 0.2784 - val_binary_accuracy: 0.9130\n",
      "Epoch 188/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3552 - binary_accuracy: 0.8860 - val_loss: 0.2777 - val_binary_accuracy: 0.9130\n",
      "Epoch 189/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3558 - binary_accuracy: 0.8860 - val_loss: 0.2776 - val_binary_accuracy: 0.9130\n",
      "Epoch 190/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3569 - binary_accuracy: 0.8860 - val_loss: 0.2798 - val_binary_accuracy: 0.9130\n",
      "Epoch 191/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3552 - binary_accuracy: 0.8860 - val_loss: 0.2757 - val_binary_accuracy: 0.9130\n",
      "Epoch 192/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3550 - binary_accuracy: 0.8824 - val_loss: 0.2809 - val_binary_accuracy: 0.9130\n",
      "Epoch 193/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3564 - binary_accuracy: 0.8824 - val_loss: 0.2815 - val_binary_accuracy: 0.9130\n",
      "Epoch 194/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3590 - binary_accuracy: 0.8750 - val_loss: 0.2753 - val_binary_accuracy: 0.9130\n",
      "Epoch 195/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3569 - binary_accuracy: 0.8860 - val_loss: 0.2919 - val_binary_accuracy: 0.9130\n",
      "Epoch 196/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3557 - binary_accuracy: 0.8787 - val_loss: 0.2692 - val_binary_accuracy: 0.9130\n",
      "Epoch 197/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3565 - binary_accuracy: 0.8750 - val_loss: 0.2739 - val_binary_accuracy: 0.9130\n",
      "Epoch 198/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3593 - binary_accuracy: 0.8824 - val_loss: 0.2860 - val_binary_accuracy: 0.9130\n",
      "Epoch 199/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3560 - binary_accuracy: 0.8824 - val_loss: 0.2681 - val_binary_accuracy: 0.9130\n",
      "Epoch 200/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3548 - binary_accuracy: 0.8824 - val_loss: 0.2786 - val_binary_accuracy: 0.9130\n",
      "Epoch 201/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3562 - binary_accuracy: 0.8824 - val_loss: 0.2788 - val_binary_accuracy: 0.9130\n",
      "Epoch 202/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3533 - binary_accuracy: 0.8824 - val_loss: 0.2727 - val_binary_accuracy: 0.9130\n",
      "Epoch 203/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3532 - binary_accuracy: 0.8824 - val_loss: 0.2778 - val_binary_accuracy: 0.9130\n",
      "Epoch 204/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3537 - binary_accuracy: 0.8860 - val_loss: 0.2801 - val_binary_accuracy: 0.9130\n",
      "Epoch 205/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3531 - binary_accuracy: 0.8824 - val_loss: 0.2769 - val_binary_accuracy: 0.9130\n",
      "Epoch 206/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3547 - binary_accuracy: 0.8860 - val_loss: 0.2825 - val_binary_accuracy: 0.9130\n",
      "Epoch 207/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3565 - binary_accuracy: 0.8787 - val_loss: 0.2737 - val_binary_accuracy: 0.9130\n",
      "Epoch 208/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3528 - binary_accuracy: 0.8860 - val_loss: 0.2838 - val_binary_accuracy: 0.9130\n",
      "Epoch 209/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3527 - binary_accuracy: 0.8860 - val_loss: 0.2757 - val_binary_accuracy: 0.9130\n",
      "Epoch 210/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3524 - binary_accuracy: 0.8860 - val_loss: 0.2780 - val_binary_accuracy: 0.9130\n",
      "Epoch 211/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3525 - binary_accuracy: 0.8824 - val_loss: 0.2750 - val_binary_accuracy: 0.9130\n",
      "Epoch 212/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3531 - binary_accuracy: 0.8824 - val_loss: 0.2749 - val_binary_accuracy: 0.9130\n",
      "Epoch 213/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3524 - binary_accuracy: 0.8824 - val_loss: 0.2828 - val_binary_accuracy: 0.9130\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x261c0f87710>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(4, input_shape=(df_train_enc_est.shape[1],), activation='elu', kernel_initializer='he_uniform'),\n",
    "    keras.layers.Dense(4, activation='swish', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(4, activation='relu', kernel_initializer='he_uniform'),\n",
    "    keras.layers.Dense(4, activation='swish', kernel_initializer='glorot_uniform'),\n",
    "    keras.layers.Dense(4, activation='sigmoid', kernel_initializer='normal'),\n",
    "    keras.layers.Dense(1, activation='hard_sigmoid', kernel_initializer='he_normal'),\n",
    "])\n",
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "callback = EarlyStopping(monitor='binary_accuracy', patience=50, restore_best_weights=True)\n",
    "\n",
    "model.fit(df_train_enc_est, y_train, validation_data = (df_test_enc_est, y_test), epochs = 500, callbacks = [callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1693253639076,
     "user": {
      "displayName": "ShadowMan",
      "userId": "06933040393855637082"
     },
     "user_tz": 420
    },
    "id": "T-zxPIzqW8jD",
    "outputId": "60ac5475-76f6-424b-f1cc-fb6d9b74fd52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "No Extraccion       1.00      0.84      0.91        38\n",
      "   Extraccion       0.84      1.00      0.91        31\n",
      "\n",
      "     accuracy                           0.91        69\n",
      "    macro avg       0.92      0.92      0.91        69\n",
      " weighted avg       0.93      0.91      0.91        69\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_names = ['No Extraccion', 'Extraccion']\n",
    "y_pred = model.predict(df_test_enc_est)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm_df = pd.DataFrame(cm, index=target_names, columns=target_names)\n",
    "sns.set(font_scale=1.0)\n",
    "\n",
    "sns.heatmap(cm_df, cmap='Blues',annot=True, fmt='g')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>edad</th>\n",
       "      <td>342.0</td>\n",
       "      <td>17.084795</td>\n",
       "      <td>6.432264</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANB</th>\n",
       "      <td>342.0</td>\n",
       "      <td>4.480994</td>\n",
       "      <td>2.721763</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICS_SN</th>\n",
       "      <td>342.0</td>\n",
       "      <td>107.242690</td>\n",
       "      <td>7.422494</td>\n",
       "      <td>83.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICS_plan_pal</th>\n",
       "      <td>342.0</td>\n",
       "      <td>64.824561</td>\n",
       "      <td>7.544098</td>\n",
       "      <td>33.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMPA</th>\n",
       "      <td>342.0</td>\n",
       "      <td>96.558480</td>\n",
       "      <td>9.570901</td>\n",
       "      <td>9.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interincisal</th>\n",
       "      <td>342.0</td>\n",
       "      <td>119.242690</td>\n",
       "      <td>10.656026</td>\n",
       "      <td>76.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>165.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lab_sup_Lin_e</th>\n",
       "      <td>342.0</td>\n",
       "      <td>-0.760234</td>\n",
       "      <td>2.364411</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lab_inf_Lin_e</th>\n",
       "      <td>342.0</td>\n",
       "      <td>0.691520</td>\n",
       "      <td>2.578507</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nasolabial</th>\n",
       "      <td>342.0</td>\n",
       "      <td>97.616959</td>\n",
       "      <td>11.228475</td>\n",
       "      <td>64.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overjet</th>\n",
       "      <td>342.0</td>\n",
       "      <td>3.422515</td>\n",
       "      <td>2.387158</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oberbite</th>\n",
       "      <td>342.0</td>\n",
       "      <td>2.473684</td>\n",
       "      <td>2.257450</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jarabak_Sum</th>\n",
       "      <td>342.0</td>\n",
       "      <td>398.125731</td>\n",
       "      <td>29.549084</td>\n",
       "      <td>380.0</td>\n",
       "      <td>393.0</td>\n",
       "      <td>397.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>931.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Angulo_perfil</th>\n",
       "      <td>342.0</td>\n",
       "      <td>165.429825</td>\n",
       "      <td>5.887463</td>\n",
       "      <td>149.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Discre_long_arco</th>\n",
       "      <td>342.0</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>1.252693</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clase_molar</th>\n",
       "      <td>342.0</td>\n",
       "      <td>1.280702</td>\n",
       "      <td>0.570673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perfil_labial</th>\n",
       "      <td>342.0</td>\n",
       "      <td>1.786550</td>\n",
       "      <td>0.997678</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>342.0</td>\n",
       "      <td>0.502924</td>\n",
       "      <td>0.500724</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  count        mean        std    min    25%    50%    75%  \\\n",
       "edad              342.0   17.084795   6.432264    9.0   13.0   15.0   18.0   \n",
       "ANB               342.0    4.480994   2.721763   -7.0    3.0    4.5    6.0   \n",
       "ICS_SN            342.0  107.242690   7.422494   83.0  102.0  107.0  111.0   \n",
       "ICS_plan_pal      342.0   64.824561   7.544098   33.0   60.0   65.0   70.0   \n",
       "IMPA              342.0   96.558480   9.570901    9.0   92.0   97.0  102.0   \n",
       "Interincisal      342.0  119.242690  10.656026   76.0  113.0  118.0  125.0   \n",
       "Lab_sup_Lin_e     342.0   -0.760234   2.364411  -11.0   -2.0   -1.0    1.0   \n",
       "Lab_inf_Lin_e     342.0    0.691520   2.578507   -9.0   -1.0    0.5    2.0   \n",
       "Nasolabial        342.0   97.616959  11.228475   64.0   90.0   98.0  105.0   \n",
       "Overjet           342.0    3.422515   2.387158   -3.0    2.0    3.0    4.0   \n",
       "Oberbite          342.0    2.473684   2.257450   -5.0    1.5    2.0    3.0   \n",
       "Jarabak_Sum       342.0  398.125731  29.549084  380.0  393.0  397.0  401.0   \n",
       "Angulo_perfil     342.0  165.429825   5.887463  149.0  162.0  166.0  169.0   \n",
       "Discre_long_arco  342.0    2.777778   1.252693    0.0    2.0    3.0    4.0   \n",
       "Clase_molar       342.0    1.280702   0.570673    0.0    1.0    1.0    2.0   \n",
       "Perfil_labial     342.0    1.786550   0.997678    0.0    1.0    2.0    2.0   \n",
       "label             342.0    0.502924   0.500724    0.0    0.0    1.0    1.0   \n",
       "\n",
       "                    max  \n",
       "edad               46.0  \n",
       "ANB                11.0  \n",
       "ICS_SN            141.0  \n",
       "ICS_plan_pal       94.0  \n",
       "IMPA              114.0  \n",
       "Interincisal      165.0  \n",
       "Lab_sup_Lin_e       6.0  \n",
       "Lab_inf_Lin_e       9.0  \n",
       "Nasolabial        130.0  \n",
       "Overjet            13.0  \n",
       "Oberbite           25.0  \n",
       "Jarabak_Sum       931.0  \n",
       "Angulo_perfil     182.0  \n",
       "Discre_long_arco    4.0  \n",
       "Clase_molar         2.0  \n",
       "Perfil_labial       4.0  \n",
       "label               1.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Base de datos FINAL_OK.csv\")\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>edad</th>\n",
       "      <td>342.0</td>\n",
       "      <td>0.371409</td>\n",
       "      <td>0.139832</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANB</th>\n",
       "      <td>342.0</td>\n",
       "      <td>0.407363</td>\n",
       "      <td>0.247433</td>\n",
       "      <td>-0.636364</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICS_SN</th>\n",
       "      <td>342.0</td>\n",
       "      <td>0.760586</td>\n",
       "      <td>0.052642</td>\n",
       "      <td>0.588652</td>\n",
       "      <td>0.723404</td>\n",
       "      <td>0.758865</td>\n",
       "      <td>0.787234</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICS_plan_pal</th>\n",
       "      <td>342.0</td>\n",
       "      <td>0.689623</td>\n",
       "      <td>0.080256</td>\n",
       "      <td>0.351064</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.691489</td>\n",
       "      <td>0.744681</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMPA</th>\n",
       "      <td>342.0</td>\n",
       "      <td>0.847004</td>\n",
       "      <td>0.083955</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.850877</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interincisal</th>\n",
       "      <td>342.0</td>\n",
       "      <td>0.722683</td>\n",
       "      <td>0.064582</td>\n",
       "      <td>0.460606</td>\n",
       "      <td>0.684848</td>\n",
       "      <td>0.715152</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lab_sup_Lin_e</th>\n",
       "      <td>342.0</td>\n",
       "      <td>-0.126706</td>\n",
       "      <td>0.394068</td>\n",
       "      <td>-1.833333</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lab_inf_Lin_e</th>\n",
       "      <td>342.0</td>\n",
       "      <td>0.076836</td>\n",
       "      <td>0.286501</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nasolabial</th>\n",
       "      <td>342.0</td>\n",
       "      <td>0.750900</td>\n",
       "      <td>0.086373</td>\n",
       "      <td>0.492308</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overjet</th>\n",
       "      <td>342.0</td>\n",
       "      <td>0.263270</td>\n",
       "      <td>0.183628</td>\n",
       "      <td>-0.230769</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oberbite</th>\n",
       "      <td>342.0</td>\n",
       "      <td>0.098947</td>\n",
       "      <td>0.090298</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jarabak_Sum</th>\n",
       "      <td>342.0</td>\n",
       "      <td>0.427632</td>\n",
       "      <td>0.031739</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.422127</td>\n",
       "      <td>0.426423</td>\n",
       "      <td>0.430720</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Angulo_perfil</th>\n",
       "      <td>342.0</td>\n",
       "      <td>0.908955</td>\n",
       "      <td>0.032349</td>\n",
       "      <td>0.818681</td>\n",
       "      <td>0.890110</td>\n",
       "      <td>0.912088</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Discre_long_arco</th>\n",
       "      <td>342.0</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.313173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clase_molar</th>\n",
       "      <td>342.0</td>\n",
       "      <td>0.640351</td>\n",
       "      <td>0.285336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perfil_labial</th>\n",
       "      <td>342.0</td>\n",
       "      <td>0.446637</td>\n",
       "      <td>0.249419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>342.0</td>\n",
       "      <td>0.502924</td>\n",
       "      <td>0.500724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  count      mean       std       min       25%       50%  \\\n",
       "edad              342.0  0.371409  0.139832  0.195652  0.282609  0.326087   \n",
       "ANB               342.0  0.407363  0.247433 -0.636364  0.272727  0.409091   \n",
       "ICS_SN            342.0  0.760586  0.052642  0.588652  0.723404  0.758865   \n",
       "ICS_plan_pal      342.0  0.689623  0.080256  0.351064  0.638298  0.691489   \n",
       "IMPA              342.0  0.847004  0.083955  0.078947  0.807018  0.850877   \n",
       "Interincisal      342.0  0.722683  0.064582  0.460606  0.684848  0.715152   \n",
       "Lab_sup_Lin_e     342.0 -0.126706  0.394068 -1.833333 -0.333333 -0.166667   \n",
       "Lab_inf_Lin_e     342.0  0.076836  0.286501 -1.000000 -0.111111  0.055556   \n",
       "Nasolabial        342.0  0.750900  0.086373  0.492308  0.692308  0.753846   \n",
       "Overjet           342.0  0.263270  0.183628 -0.230769  0.153846  0.230769   \n",
       "Oberbite          342.0  0.098947  0.090298 -0.200000  0.060000  0.080000   \n",
       "Jarabak_Sum       342.0  0.427632  0.031739  0.408163  0.422127  0.426423   \n",
       "Angulo_perfil     342.0  0.908955  0.032349  0.818681  0.890110  0.912088   \n",
       "Discre_long_arco  342.0  0.694444  0.313173  0.000000  0.500000  0.750000   \n",
       "Clase_molar       342.0  0.640351  0.285336  0.000000  0.500000  0.500000   \n",
       "Perfil_labial     342.0  0.446637  0.249419  0.000000  0.250000  0.500000   \n",
       "label             342.0  0.502924  0.500724  0.000000  0.000000  1.000000   \n",
       "\n",
       "                       75%  max  \n",
       "edad              0.391304  1.0  \n",
       "ANB               0.545455  1.0  \n",
       "ICS_SN            0.787234  1.0  \n",
       "ICS_plan_pal      0.744681  1.0  \n",
       "IMPA              0.894737  1.0  \n",
       "Interincisal      0.757576  1.0  \n",
       "Lab_sup_Lin_e     0.166667  1.0  \n",
       "Lab_inf_Lin_e     0.222222  1.0  \n",
       "Nasolabial        0.807692  1.0  \n",
       "Overjet           0.307692  1.0  \n",
       "Oberbite          0.120000  1.0  \n",
       "Jarabak_Sum       0.430720  1.0  \n",
       "Angulo_perfil     0.928571  1.0  \n",
       "Discre_long_arco  1.000000  1.0  \n",
       "Clase_molar       1.000000  1.0  \n",
       "Perfil_labial     0.500000  1.0  \n",
       "label             1.000000  1.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['edad'] = df['edad'] / 46\n",
    "df['ANB'] = df['ANB'] / 11\n",
    "df['ICS_SN'] = df['ICS_SN'] / 141\n",
    "df['ICS_plan_pal'] = df['ICS_plan_pal'] / 94\n",
    "df['IMPA'] = df['IMPA'] / 114\n",
    "df['Interincisal'] = df['Interincisal'] / 165\n",
    "df['Lab_sup_Lin_e'] = df['Lab_sup_Lin_e'] / 6\n",
    "df['Lab_inf_Lin_e'] = df['Lab_inf_Lin_e'] / 9\n",
    "df['Nasolabial'] = df['Nasolabial'] / 130\n",
    "df['Overjet'] = df['Overjet'] / 13\n",
    "df['Oberbite'] = df['Oberbite'] / 25\n",
    "df['Jarabak_Sum'] = df['Jarabak_Sum'] / 931\n",
    "df['Angulo_perfil'] = df['Angulo_perfil'] / 182\n",
    "df['Discre_long_arco'] = df['Discre_long_arco'] / 4\n",
    "df['Clase_molar'] = df['Clase_molar'] / 2\n",
    "df['Perfil_labial'] = df['Perfil_labial'] / 4\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edad</th>\n",
       "      <th>genero</th>\n",
       "      <th>ANB</th>\n",
       "      <th>ICS_SN</th>\n",
       "      <th>ICS_plan_pal</th>\n",
       "      <th>IMPA</th>\n",
       "      <th>Interincisal</th>\n",
       "      <th>Lab_sup_Lin_e</th>\n",
       "      <th>Lab_inf_Lin_e</th>\n",
       "      <th>Nasolabial</th>\n",
       "      <th>Overjet</th>\n",
       "      <th>Oberbite</th>\n",
       "      <th>Jarabak_Sum</th>\n",
       "      <th>Angulo_perfil</th>\n",
       "      <th>Discre_long_arco</th>\n",
       "      <th>Clase_molar</th>\n",
       "      <th>Perfil_labial</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.673913</td>\n",
       "      <td>M</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.751773</td>\n",
       "      <td>0.744681</td>\n",
       "      <td>0.859649</td>\n",
       "      <td>0.769697</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.715385</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.945055</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.282609</td>\n",
       "      <td>M</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.716312</td>\n",
       "      <td>0.744681</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.836364</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.890110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.282609</td>\n",
       "      <td>M</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.673759</td>\n",
       "      <td>0.744681</td>\n",
       "      <td>0.719298</td>\n",
       "      <td>0.860606</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.222222</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.432868</td>\n",
       "      <td>0.895604</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.260870</td>\n",
       "      <td>F</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.673759</td>\n",
       "      <td>0.861702</td>\n",
       "      <td>0.780702</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.426423</td>\n",
       "      <td>0.906593</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.304348</td>\n",
       "      <td>F</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.055556</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.415682</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       edad genero       ANB    ICS_SN  ICS_plan_pal      IMPA  Interincisal  \\\n",
       "0  0.673913      M  0.090909  0.751773      0.744681  0.859649      0.769697   \n",
       "1  0.282609      M  0.454545  0.716312      0.744681  0.807018      0.836364   \n",
       "2  0.282609      M  0.272727  0.673759      0.744681  0.719298      0.860606   \n",
       "3  0.260870      F  0.681818  0.673759      0.861702  0.780702      0.854545   \n",
       "4  0.304348      F  0.454545  0.808511      0.638298  0.921053      0.696970   \n",
       "\n",
       "   Lab_sup_Lin_e  Lab_inf_Lin_e  Nasolabial   Overjet  Oberbite  Jarabak_Sum  \\\n",
       "0      -0.333333       0.000000    0.715385  0.307692      0.16     0.421053   \n",
       "1       0.333333       0.000000    0.707692  0.230769      0.08     0.421053   \n",
       "2      -0.500000      -0.222222    0.800000  0.384615      0.32     0.432868   \n",
       "3       0.000000       0.000000    0.730769  0.269231      0.20     0.426423   \n",
       "4      -0.166667      -0.055556    0.700000  0.230769      0.06     0.415682   \n",
       "\n",
       "   Angulo_perfil  Discre_long_arco  Clase_molar  Perfil_labial  label  \n",
       "0       0.945055              1.00          0.5           0.25      0  \n",
       "1       0.890110              0.50          0.5           0.50      0  \n",
       "2       0.895604              0.50          1.0           0.25      0  \n",
       "3       0.906593              0.50          1.0           0.25      0  \n",
       "4       0.923077              0.25          0.5           0.25      0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((273, 17), (69, 17))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = utils.shuffle(df,random_state=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.drop(['label'],axis=1),df['label'],test_size=0.2,random_state=42)\n",
    "x_train.shape,x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 273 entries, 328 to 105\n",
      "Data columns (total 17 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   edad              273 non-null    float64\n",
      " 1   genero            273 non-null    float64\n",
      " 2   ANB               273 non-null    float64\n",
      " 3   ICS_SN            273 non-null    float64\n",
      " 4   ICS_plan_pal      273 non-null    float64\n",
      " 5   IMPA              273 non-null    float64\n",
      " 6   Interincisal      273 non-null    float64\n",
      " 7   Lab_sup_Lin_e     273 non-null    float64\n",
      " 8   Lab_inf_Lin_e     273 non-null    float64\n",
      " 9   Nasolabial        273 non-null    float64\n",
      " 10  Overjet           273 non-null    float64\n",
      " 11  Oberbite          273 non-null    float64\n",
      " 12  Jarabak_Sum       273 non-null    float64\n",
      " 13  Angulo_perfil     273 non-null    float64\n",
      " 14  Discre_long_arco  273 non-null    float64\n",
      " 15  Clase_molar       273 non-null    float64\n",
      " 16  Perfil_labial     273 non-null    float64\n",
      "dtypes: float64(17)\n",
      "memory usage: 38.4 KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edad</th>\n",
       "      <th>genero</th>\n",
       "      <th>ANB</th>\n",
       "      <th>ICS_SN</th>\n",
       "      <th>ICS_plan_pal</th>\n",
       "      <th>IMPA</th>\n",
       "      <th>Interincisal</th>\n",
       "      <th>Lab_sup_Lin_e</th>\n",
       "      <th>Lab_inf_Lin_e</th>\n",
       "      <th>Nasolabial</th>\n",
       "      <th>Overjet</th>\n",
       "      <th>Oberbite</th>\n",
       "      <th>Jarabak_Sum</th>\n",
       "      <th>Angulo_perfil</th>\n",
       "      <th>Discre_long_arco</th>\n",
       "      <th>Clase_molar</th>\n",
       "      <th>Perfil_labial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.431384</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.751773</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>0.885965</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.419979</td>\n",
       "      <td>0.895604</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.684624</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.497076</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>0.659574</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.703030</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>0.669231</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.424275</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.343558</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.431384</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.886525</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>0.877193</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.950549</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.684624</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>0.282609</td>\n",
       "      <td>0.431384</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.794326</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.723077</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.419979</td>\n",
       "      <td>0.901099</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.343558</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.431384</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.801418</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.642424</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.715385</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.431794</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.684624</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         edad    genero       ANB    ICS_SN  ICS_plan_pal      IMPA  \\\n",
       "328  0.369565  0.431384  0.636364  0.751773      0.712766  0.885965   \n",
       "99   0.347826  0.497076  0.181818  0.765957      0.659574  0.912281   \n",
       "257  0.260870  0.431384  0.272727  0.886525      0.531915  0.877193   \n",
       "287  0.282609  0.431384  0.272727  0.794326      0.638298  0.929825   \n",
       "248  0.260870  0.431384  0.454545  0.801418      0.638298  0.894737   \n",
       "\n",
       "     Interincisal  Lab_sup_Lin_e  Lab_inf_Lin_e  Nasolabial   Overjet  \\\n",
       "328      0.727273      -0.250000       0.166667    0.846154  0.230769   \n",
       "99       0.703030      -0.250000      -0.111111    0.669231  0.076923   \n",
       "257      0.636364      -0.166667       0.000000    0.692308  0.615385   \n",
       "287      0.666667       0.333333       0.333333    0.723077  0.307692   \n",
       "248      0.642424       0.166667       0.111111    0.715385  0.384615   \n",
       "\n",
       "     Oberbite  Jarabak_Sum  Angulo_perfil  Discre_long_arco  Clase_molar  \\\n",
       "328      0.08     0.419979       0.895604              1.00     0.684624   \n",
       "99       0.08     0.424275       0.928571              0.00     0.343558   \n",
       "257      0.12     0.421053       0.950549              0.50     0.684624   \n",
       "287      0.12     0.419979       0.901099              0.75     0.343558   \n",
       "248      0.12     0.431794       0.923077              1.00     0.684624   \n",
       "\n",
       "     Perfil_labial  \n",
       "328           0.50  \n",
       "99            0.25  \n",
       "257           0.25  \n",
       "287           0.75  \n",
       "248           0.50  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "encoder = TargetEncoder(cols=['genero','Clase_molar'])\n",
    "encoder.fit(x_train,y_train)\n",
    "\n",
    "#Utiliza el mismo encoder para transformar el dataset de testing y el de testing?\n",
    "df_train_enc = encoder.transform(x_train)\n",
    "df_test_enc = encoder.transform(x_test)\n",
    "print(df_train_enc.info())\n",
    "df_train_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "9/9 [==============================] - 2s 37ms/step - loss: 0.7026 - binary_accuracy: 0.4725 - val_loss: 0.6751 - val_binary_accuracy: 0.6232\n",
      "Epoch 2/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.7013 - binary_accuracy: 0.4725 - val_loss: 0.6764 - val_binary_accuracy: 0.6232\n",
      "Epoch 3/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.7002 - binary_accuracy: 0.4725 - val_loss: 0.6779 - val_binary_accuracy: 0.6232\n",
      "Epoch 4/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.6988 - binary_accuracy: 0.4725 - val_loss: 0.6794 - val_binary_accuracy: 0.6232\n",
      "Epoch 5/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6980 - binary_accuracy: 0.4725 - val_loss: 0.6811 - val_binary_accuracy: 0.6232\n",
      "Epoch 6/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.6968 - binary_accuracy: 0.4725 - val_loss: 0.6822 - val_binary_accuracy: 0.6232\n",
      "Epoch 7/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6959 - binary_accuracy: 0.4725 - val_loss: 0.6831 - val_binary_accuracy: 0.6232\n",
      "Epoch 8/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6950 - binary_accuracy: 0.4725 - val_loss: 0.6839 - val_binary_accuracy: 0.6232\n",
      "Epoch 9/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6942 - binary_accuracy: 0.4725 - val_loss: 0.6843 - val_binary_accuracy: 0.6232\n",
      "Epoch 10/500\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 0.6933 - binary_accuracy: 0.4725 - val_loss: 0.6841 - val_binary_accuracy: 0.6232\n",
      "Epoch 11/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.6922 - binary_accuracy: 0.4725 - val_loss: 0.6844 - val_binary_accuracy: 0.6232\n",
      "Epoch 12/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6910 - binary_accuracy: 0.4725 - val_loss: 0.6833 - val_binary_accuracy: 0.6232\n",
      "Epoch 13/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.6897 - binary_accuracy: 0.5128 - val_loss: 0.6818 - val_binary_accuracy: 0.7536\n",
      "Epoch 14/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6880 - binary_accuracy: 0.5897 - val_loss: 0.6799 - val_binary_accuracy: 0.7391\n",
      "Epoch 15/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6864 - binary_accuracy: 0.6264 - val_loss: 0.6761 - val_binary_accuracy: 0.7536\n",
      "Epoch 16/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.6841 - binary_accuracy: 0.6777 - val_loss: 0.6741 - val_binary_accuracy: 0.8116\n",
      "Epoch 17/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6819 - binary_accuracy: 0.6850 - val_loss: 0.6721 - val_binary_accuracy: 0.7971\n",
      "Epoch 18/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6792 - binary_accuracy: 0.6886 - val_loss: 0.6668 - val_binary_accuracy: 0.7826\n",
      "Epoch 19/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6760 - binary_accuracy: 0.6923 - val_loss: 0.6619 - val_binary_accuracy: 0.7826\n",
      "Epoch 20/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6727 - binary_accuracy: 0.7106 - val_loss: 0.6584 - val_binary_accuracy: 0.7826\n",
      "Epoch 21/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6689 - binary_accuracy: 0.7179 - val_loss: 0.6510 - val_binary_accuracy: 0.7971\n",
      "Epoch 22/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6651 - binary_accuracy: 0.6996 - val_loss: 0.6446 - val_binary_accuracy: 0.7826\n",
      "Epoch 23/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.6611 - binary_accuracy: 0.7179 - val_loss: 0.6421 - val_binary_accuracy: 0.7681\n",
      "Epoch 24/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6565 - binary_accuracy: 0.7216 - val_loss: 0.6340 - val_binary_accuracy: 0.7826\n",
      "Epoch 25/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.6521 - binary_accuracy: 0.7070 - val_loss: 0.6314 - val_binary_accuracy: 0.7536\n",
      "Epoch 26/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6469 - binary_accuracy: 0.7033 - val_loss: 0.6237 - val_binary_accuracy: 0.7681\n",
      "Epoch 27/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.6420 - binary_accuracy: 0.6996 - val_loss: 0.6178 - val_binary_accuracy: 0.7536\n",
      "Epoch 28/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6365 - binary_accuracy: 0.7106 - val_loss: 0.6150 - val_binary_accuracy: 0.7681\n",
      "Epoch 29/500\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.6308 - binary_accuracy: 0.7216 - val_loss: 0.6057 - val_binary_accuracy: 0.7536\n",
      "Epoch 30/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6254 - binary_accuracy: 0.7106 - val_loss: 0.5987 - val_binary_accuracy: 0.7536\n",
      "Epoch 31/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6193 - binary_accuracy: 0.7253 - val_loss: 0.5944 - val_binary_accuracy: 0.7681\n",
      "Epoch 32/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6140 - binary_accuracy: 0.7253 - val_loss: 0.5875 - val_binary_accuracy: 0.7681\n",
      "Epoch 33/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6086 - binary_accuracy: 0.7326 - val_loss: 0.5877 - val_binary_accuracy: 0.7536\n",
      "Epoch 34/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.6029 - binary_accuracy: 0.7253 - val_loss: 0.5714 - val_binary_accuracy: 0.7971\n",
      "Epoch 35/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5962 - binary_accuracy: 0.7253 - val_loss: 0.5707 - val_binary_accuracy: 0.7681\n",
      "Epoch 36/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5905 - binary_accuracy: 0.7326 - val_loss: 0.5611 - val_binary_accuracy: 0.7826\n",
      "Epoch 37/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5846 - binary_accuracy: 0.7436 - val_loss: 0.5546 - val_binary_accuracy: 0.7826\n",
      "Epoch 38/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5790 - binary_accuracy: 0.7473 - val_loss: 0.5528 - val_binary_accuracy: 0.7826\n",
      "Epoch 39/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5739 - binary_accuracy: 0.7473 - val_loss: 0.5401 - val_binary_accuracy: 0.7971\n",
      "Epoch 40/500\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.5687 - binary_accuracy: 0.7619 - val_loss: 0.5335 - val_binary_accuracy: 0.7826\n",
      "Epoch 41/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.5625 - binary_accuracy: 0.7582 - val_loss: 0.5182 - val_binary_accuracy: 0.8261\n",
      "Epoch 42/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.5578 - binary_accuracy: 0.7619 - val_loss: 0.5202 - val_binary_accuracy: 0.7971\n",
      "Epoch 43/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5529 - binary_accuracy: 0.7692 - val_loss: 0.5171 - val_binary_accuracy: 0.7971\n",
      "Epoch 44/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5480 - binary_accuracy: 0.7802 - val_loss: 0.5059 - val_binary_accuracy: 0.8116\n",
      "Epoch 45/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.5435 - binary_accuracy: 0.7802 - val_loss: 0.4956 - val_binary_accuracy: 0.8261\n",
      "Epoch 46/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5393 - binary_accuracy: 0.7729 - val_loss: 0.4977 - val_binary_accuracy: 0.8116\n",
      "Epoch 47/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5364 - binary_accuracy: 0.7802 - val_loss: 0.4872 - val_binary_accuracy: 0.8261\n",
      "Epoch 48/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5315 - binary_accuracy: 0.7802 - val_loss: 0.4748 - val_binary_accuracy: 0.8261\n",
      "Epoch 49/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5287 - binary_accuracy: 0.7875 - val_loss: 0.4798 - val_binary_accuracy: 0.8261\n",
      "Epoch 50/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.5249 - binary_accuracy: 0.7875 - val_loss: 0.4663 - val_binary_accuracy: 0.8261\n",
      "Epoch 51/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5216 - binary_accuracy: 0.7875 - val_loss: 0.4697 - val_binary_accuracy: 0.8261\n",
      "Epoch 52/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5205 - binary_accuracy: 0.7875 - val_loss: 0.4635 - val_binary_accuracy: 0.8261\n",
      "Epoch 53/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5160 - binary_accuracy: 0.7949 - val_loss: 0.4534 - val_binary_accuracy: 0.8406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.5130 - binary_accuracy: 0.7949 - val_loss: 0.4526 - val_binary_accuracy: 0.8406\n",
      "Epoch 55/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5118 - binary_accuracy: 0.7912 - val_loss: 0.4452 - val_binary_accuracy: 0.8406\n",
      "Epoch 56/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5087 - binary_accuracy: 0.7949 - val_loss: 0.4480 - val_binary_accuracy: 0.8406\n",
      "Epoch 57/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5059 - binary_accuracy: 0.7985 - val_loss: 0.4430 - val_binary_accuracy: 0.8406\n",
      "Epoch 58/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5040 - binary_accuracy: 0.7985 - val_loss: 0.4309 - val_binary_accuracy: 0.8551\n",
      "Epoch 59/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.5020 - binary_accuracy: 0.7985 - val_loss: 0.4371 - val_binary_accuracy: 0.8406\n",
      "Epoch 60/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.5008 - binary_accuracy: 0.8022 - val_loss: 0.4335 - val_binary_accuracy: 0.8406\n",
      "Epoch 61/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4982 - binary_accuracy: 0.8022 - val_loss: 0.4215 - val_binary_accuracy: 0.8551\n",
      "Epoch 62/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4962 - binary_accuracy: 0.7985 - val_loss: 0.4294 - val_binary_accuracy: 0.8406\n",
      "Epoch 63/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4956 - binary_accuracy: 0.8059 - val_loss: 0.4195 - val_binary_accuracy: 0.8551\n",
      "Epoch 64/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4919 - binary_accuracy: 0.8022 - val_loss: 0.4256 - val_binary_accuracy: 0.8406\n",
      "Epoch 65/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4920 - binary_accuracy: 0.7985 - val_loss: 0.4125 - val_binary_accuracy: 0.8551\n",
      "Epoch 66/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4914 - binary_accuracy: 0.8059 - val_loss: 0.4246 - val_binary_accuracy: 0.8406\n",
      "Epoch 67/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4865 - binary_accuracy: 0.8059 - val_loss: 0.4088 - val_binary_accuracy: 0.8551\n",
      "Epoch 68/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4874 - binary_accuracy: 0.7875 - val_loss: 0.3989 - val_binary_accuracy: 0.8841\n",
      "Epoch 69/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4888 - binary_accuracy: 0.7949 - val_loss: 0.4170 - val_binary_accuracy: 0.8551\n",
      "Epoch 70/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4830 - binary_accuracy: 0.8059 - val_loss: 0.3957 - val_binary_accuracy: 0.8841\n",
      "Epoch 71/500\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.4879 - binary_accuracy: 0.8022 - val_loss: 0.4139 - val_binary_accuracy: 0.8551\n",
      "Epoch 72/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4778 - binary_accuracy: 0.8168 - val_loss: 0.3867 - val_binary_accuracy: 0.8841\n",
      "Epoch 73/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4814 - binary_accuracy: 0.7839 - val_loss: 0.3909 - val_binary_accuracy: 0.8696\n",
      "Epoch 74/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4837 - binary_accuracy: 0.7949 - val_loss: 0.4173 - val_binary_accuracy: 0.8406\n",
      "Epoch 75/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4768 - binary_accuracy: 0.8132 - val_loss: 0.3859 - val_binary_accuracy: 0.8841\n",
      "Epoch 76/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4743 - binary_accuracy: 0.7985 - val_loss: 0.3891 - val_binary_accuracy: 0.8696\n",
      "Epoch 77/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4719 - binary_accuracy: 0.8022 - val_loss: 0.3957 - val_binary_accuracy: 0.8551\n",
      "Epoch 78/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4706 - binary_accuracy: 0.8022 - val_loss: 0.3924 - val_binary_accuracy: 0.8551\n",
      "Epoch 79/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4701 - binary_accuracy: 0.8022 - val_loss: 0.3865 - val_binary_accuracy: 0.8696\n",
      "Epoch 80/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4676 - binary_accuracy: 0.8022 - val_loss: 0.3820 - val_binary_accuracy: 0.8696\n",
      "Epoch 81/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4658 - binary_accuracy: 0.8059 - val_loss: 0.3792 - val_binary_accuracy: 0.8696\n",
      "Epoch 82/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4650 - binary_accuracy: 0.7985 - val_loss: 0.3751 - val_binary_accuracy: 0.8841\n",
      "Epoch 83/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4639 - binary_accuracy: 0.8022 - val_loss: 0.3793 - val_binary_accuracy: 0.8696\n",
      "Epoch 84/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4633 - binary_accuracy: 0.8022 - val_loss: 0.3743 - val_binary_accuracy: 0.8696\n",
      "Epoch 85/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4608 - binary_accuracy: 0.8022 - val_loss: 0.3695 - val_binary_accuracy: 0.8841\n",
      "Epoch 86/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4587 - binary_accuracy: 0.8132 - val_loss: 0.3749 - val_binary_accuracy: 0.8696\n",
      "Epoch 87/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4584 - binary_accuracy: 0.8095 - val_loss: 0.3669 - val_binary_accuracy: 0.8696\n",
      "Epoch 88/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4567 - binary_accuracy: 0.8059 - val_loss: 0.3628 - val_binary_accuracy: 0.8841\n",
      "Epoch 89/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4542 - binary_accuracy: 0.8095 - val_loss: 0.3650 - val_binary_accuracy: 0.8696\n",
      "Epoch 90/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4565 - binary_accuracy: 0.8205 - val_loss: 0.3656 - val_binary_accuracy: 0.8696\n",
      "Epoch 91/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4517 - binary_accuracy: 0.8205 - val_loss: 0.3552 - val_binary_accuracy: 0.8841\n",
      "Epoch 92/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4529 - binary_accuracy: 0.8132 - val_loss: 0.3509 - val_binary_accuracy: 0.8841\n",
      "Epoch 93/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4554 - binary_accuracy: 0.8132 - val_loss: 0.3668 - val_binary_accuracy: 0.8696\n",
      "Epoch 94/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4488 - binary_accuracy: 0.8242 - val_loss: 0.3497 - val_binary_accuracy: 0.8841\n",
      "Epoch 95/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4475 - binary_accuracy: 0.8242 - val_loss: 0.3443 - val_binary_accuracy: 0.8841\n",
      "Epoch 96/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4482 - binary_accuracy: 0.8242 - val_loss: 0.3446 - val_binary_accuracy: 0.8841\n",
      "Epoch 97/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4434 - binary_accuracy: 0.8278 - val_loss: 0.3554 - val_binary_accuracy: 0.8696\n",
      "Epoch 98/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4442 - binary_accuracy: 0.8315 - val_loss: 0.3448 - val_binary_accuracy: 0.8841\n",
      "Epoch 99/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4413 - binary_accuracy: 0.8278 - val_loss: 0.3444 - val_binary_accuracy: 0.8841\n",
      "Epoch 100/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4407 - binary_accuracy: 0.8352 - val_loss: 0.3388 - val_binary_accuracy: 0.8841\n",
      "Epoch 101/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4414 - binary_accuracy: 0.8278 - val_loss: 0.3418 - val_binary_accuracy: 0.8841\n",
      "Epoch 102/500\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.4394 - binary_accuracy: 0.8278 - val_loss: 0.3340 - val_binary_accuracy: 0.8841\n",
      "Epoch 103/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4365 - binary_accuracy: 0.8352 - val_loss: 0.3352 - val_binary_accuracy: 0.8841\n",
      "Epoch 104/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4366 - binary_accuracy: 0.8315 - val_loss: 0.3382 - val_binary_accuracy: 0.8986\n",
      "Epoch 105/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4359 - binary_accuracy: 0.8388 - val_loss: 0.3296 - val_binary_accuracy: 0.8986\n",
      "Epoch 106/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4331 - binary_accuracy: 0.8388 - val_loss: 0.3302 - val_binary_accuracy: 0.8986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4343 - binary_accuracy: 0.8352 - val_loss: 0.3222 - val_binary_accuracy: 0.8841\n",
      "Epoch 108/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4335 - binary_accuracy: 0.8352 - val_loss: 0.3253 - val_binary_accuracy: 0.8986\n",
      "Epoch 109/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4328 - binary_accuracy: 0.8388 - val_loss: 0.3219 - val_binary_accuracy: 0.8986\n",
      "Epoch 110/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4297 - binary_accuracy: 0.8388 - val_loss: 0.3143 - val_binary_accuracy: 0.8986\n",
      "Epoch 111/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4298 - binary_accuracy: 0.8352 - val_loss: 0.3156 - val_binary_accuracy: 0.8986\n",
      "Epoch 112/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4288 - binary_accuracy: 0.8425 - val_loss: 0.3227 - val_binary_accuracy: 0.9130\n",
      "Epoch 113/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4255 - binary_accuracy: 0.8388 - val_loss: 0.3090 - val_binary_accuracy: 0.8986\n",
      "Epoch 114/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4270 - binary_accuracy: 0.8388 - val_loss: 0.3089 - val_binary_accuracy: 0.8986\n",
      "Epoch 115/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4261 - binary_accuracy: 0.8462 - val_loss: 0.3205 - val_binary_accuracy: 0.9130\n",
      "Epoch 116/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4247 - binary_accuracy: 0.8425 - val_loss: 0.3056 - val_binary_accuracy: 0.8986\n",
      "Epoch 117/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4235 - binary_accuracy: 0.8425 - val_loss: 0.3075 - val_binary_accuracy: 0.8986\n",
      "Epoch 118/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4225 - binary_accuracy: 0.8425 - val_loss: 0.3089 - val_binary_accuracy: 0.9130\n",
      "Epoch 119/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4219 - binary_accuracy: 0.8425 - val_loss: 0.3042 - val_binary_accuracy: 0.8986\n",
      "Epoch 120/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4226 - binary_accuracy: 0.8462 - val_loss: 0.2967 - val_binary_accuracy: 0.8986\n",
      "Epoch 121/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4189 - binary_accuracy: 0.8462 - val_loss: 0.3085 - val_binary_accuracy: 0.9130\n",
      "Epoch 122/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4184 - binary_accuracy: 0.8425 - val_loss: 0.2999 - val_binary_accuracy: 0.8986\n",
      "Epoch 123/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4198 - binary_accuracy: 0.8462 - val_loss: 0.2915 - val_binary_accuracy: 0.8986\n",
      "Epoch 124/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4190 - binary_accuracy: 0.8425 - val_loss: 0.2963 - val_binary_accuracy: 0.8986\n",
      "Epoch 125/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4157 - binary_accuracy: 0.8425 - val_loss: 0.2910 - val_binary_accuracy: 0.8986\n",
      "Epoch 126/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4170 - binary_accuracy: 0.8425 - val_loss: 0.2909 - val_binary_accuracy: 0.8986\n",
      "Epoch 127/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4151 - binary_accuracy: 0.8462 - val_loss: 0.3005 - val_binary_accuracy: 0.9130\n",
      "Epoch 128/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4138 - binary_accuracy: 0.8425 - val_loss: 0.2853 - val_binary_accuracy: 0.8986\n",
      "Epoch 129/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4129 - binary_accuracy: 0.8498 - val_loss: 0.2885 - val_binary_accuracy: 0.8986\n",
      "Epoch 130/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4155 - binary_accuracy: 0.8498 - val_loss: 0.2993 - val_binary_accuracy: 0.8986\n",
      "Epoch 131/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4129 - binary_accuracy: 0.8571 - val_loss: 0.2791 - val_binary_accuracy: 0.8986\n",
      "Epoch 132/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4166 - binary_accuracy: 0.8535 - val_loss: 0.2946 - val_binary_accuracy: 0.8986\n",
      "Epoch 133/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4077 - binary_accuracy: 0.8462 - val_loss: 0.2751 - val_binary_accuracy: 0.8986\n",
      "Epoch 134/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4102 - binary_accuracy: 0.8535 - val_loss: 0.2759 - val_binary_accuracy: 0.8986\n",
      "Epoch 135/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4067 - binary_accuracy: 0.8498 - val_loss: 0.2854 - val_binary_accuracy: 0.9130\n",
      "Epoch 136/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4075 - binary_accuracy: 0.8498 - val_loss: 0.2723 - val_binary_accuracy: 0.8986\n",
      "Epoch 137/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4054 - binary_accuracy: 0.8608 - val_loss: 0.2764 - val_binary_accuracy: 0.9130\n",
      "Epoch 138/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4044 - binary_accuracy: 0.8535 - val_loss: 0.2815 - val_binary_accuracy: 0.9130\n",
      "Epoch 139/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4048 - binary_accuracy: 0.8498 - val_loss: 0.2751 - val_binary_accuracy: 0.9130\n",
      "Epoch 140/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4046 - binary_accuracy: 0.8535 - val_loss: 0.2669 - val_binary_accuracy: 0.9275\n",
      "Epoch 141/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4027 - binary_accuracy: 0.8535 - val_loss: 0.2772 - val_binary_accuracy: 0.9130\n",
      "Epoch 142/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4029 - binary_accuracy: 0.8608 - val_loss: 0.2654 - val_binary_accuracy: 0.9130\n",
      "Epoch 143/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4016 - binary_accuracy: 0.8571 - val_loss: 0.2708 - val_binary_accuracy: 0.9130\n",
      "Epoch 144/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3998 - binary_accuracy: 0.8608 - val_loss: 0.2610 - val_binary_accuracy: 0.9275\n",
      "Epoch 145/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3988 - binary_accuracy: 0.8608 - val_loss: 0.2647 - val_binary_accuracy: 0.9130\n",
      "Epoch 146/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3983 - binary_accuracy: 0.8608 - val_loss: 0.2684 - val_binary_accuracy: 0.9130\n",
      "Epoch 147/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3967 - binary_accuracy: 0.8681 - val_loss: 0.2609 - val_binary_accuracy: 0.9420\n",
      "Epoch 148/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3978 - binary_accuracy: 0.8645 - val_loss: 0.2629 - val_binary_accuracy: 0.9130\n",
      "Epoch 149/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3993 - binary_accuracy: 0.8645 - val_loss: 0.2583 - val_binary_accuracy: 0.9420\n",
      "Epoch 150/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3959 - binary_accuracy: 0.8681 - val_loss: 0.2657 - val_binary_accuracy: 0.9130\n",
      "Epoch 151/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3956 - binary_accuracy: 0.8681 - val_loss: 0.2659 - val_binary_accuracy: 0.9275\n",
      "Epoch 152/500\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.3961 - binary_accuracy: 0.8681 - val_loss: 0.2587 - val_binary_accuracy: 0.9420\n",
      "Epoch 153/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3945 - binary_accuracy: 0.8681 - val_loss: 0.2614 - val_binary_accuracy: 0.9275\n",
      "Epoch 154/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3976 - binary_accuracy: 0.8681 - val_loss: 0.2519 - val_binary_accuracy: 0.9275\n",
      "Epoch 155/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3918 - binary_accuracy: 0.8681 - val_loss: 0.2602 - val_binary_accuracy: 0.9275\n",
      "Epoch 156/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3939 - binary_accuracy: 0.8645 - val_loss: 0.2578 - val_binary_accuracy: 0.9275\n",
      "Epoch 157/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3901 - binary_accuracy: 0.8718 - val_loss: 0.2502 - val_binary_accuracy: 0.9275\n",
      "Epoch 158/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3919 - binary_accuracy: 0.8645 - val_loss: 0.2519 - val_binary_accuracy: 0.9275\n",
      "Epoch 159/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3884 - binary_accuracy: 0.8718 - val_loss: 0.2597 - val_binary_accuracy: 0.9275\n",
      "Epoch 160/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3918 - binary_accuracy: 0.8681 - val_loss: 0.2510 - val_binary_accuracy: 0.9275\n",
      "Epoch 161/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3901 - binary_accuracy: 0.8718 - val_loss: 0.2515 - val_binary_accuracy: 0.9420\n",
      "Epoch 162/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3888 - binary_accuracy: 0.8718 - val_loss: 0.2546 - val_binary_accuracy: 0.9420\n",
      "Epoch 163/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3888 - binary_accuracy: 0.8681 - val_loss: 0.2455 - val_binary_accuracy: 0.9275\n",
      "Epoch 164/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3892 - binary_accuracy: 0.8755 - val_loss: 0.2522 - val_binary_accuracy: 0.9420\n",
      "Epoch 165/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3885 - binary_accuracy: 0.8681 - val_loss: 0.2512 - val_binary_accuracy: 0.9420\n",
      "Epoch 166/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3858 - binary_accuracy: 0.8755 - val_loss: 0.2436 - val_binary_accuracy: 0.9275\n",
      "Epoch 167/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3888 - binary_accuracy: 0.8681 - val_loss: 0.2423 - val_binary_accuracy: 0.9275\n",
      "Epoch 168/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3875 - binary_accuracy: 0.8718 - val_loss: 0.2535 - val_binary_accuracy: 0.9275\n",
      "Epoch 169/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3857 - binary_accuracy: 0.8718 - val_loss: 0.2439 - val_binary_accuracy: 0.9420\n",
      "Epoch 170/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3865 - binary_accuracy: 0.8791 - val_loss: 0.2430 - val_binary_accuracy: 0.9420\n",
      "Epoch 171/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3853 - binary_accuracy: 0.8755 - val_loss: 0.2450 - val_binary_accuracy: 0.9420\n",
      "Epoch 172/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3927 - binary_accuracy: 0.8645 - val_loss: 0.2482 - val_binary_accuracy: 0.9420\n",
      "Epoch 173/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3825 - binary_accuracy: 0.8755 - val_loss: 0.2396 - val_binary_accuracy: 0.9275\n",
      "Epoch 174/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3862 - binary_accuracy: 0.8718 - val_loss: 0.2473 - val_binary_accuracy: 0.9420\n",
      "Epoch 175/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3865 - binary_accuracy: 0.8645 - val_loss: 0.2497 - val_binary_accuracy: 0.9420\n",
      "Epoch 176/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3830 - binary_accuracy: 0.8755 - val_loss: 0.2419 - val_binary_accuracy: 0.9275\n",
      "Epoch 177/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3835 - binary_accuracy: 0.8791 - val_loss: 0.2452 - val_binary_accuracy: 0.9275\n",
      "Epoch 178/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3840 - binary_accuracy: 0.8791 - val_loss: 0.2438 - val_binary_accuracy: 0.9275\n",
      "Epoch 179/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3867 - binary_accuracy: 0.8681 - val_loss: 0.2488 - val_binary_accuracy: 0.9420\n",
      "Epoch 180/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3855 - binary_accuracy: 0.8718 - val_loss: 0.2390 - val_binary_accuracy: 0.9275\n",
      "Epoch 181/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3816 - binary_accuracy: 0.8791 - val_loss: 0.2444 - val_binary_accuracy: 0.9420\n",
      "Epoch 182/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3842 - binary_accuracy: 0.8718 - val_loss: 0.2458 - val_binary_accuracy: 0.9420\n",
      "Epoch 183/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3818 - binary_accuracy: 0.8755 - val_loss: 0.2413 - val_binary_accuracy: 0.9275\n",
      "Epoch 184/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3843 - binary_accuracy: 0.8718 - val_loss: 0.2482 - val_binary_accuracy: 0.9420\n",
      "Epoch 185/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3795 - binary_accuracy: 0.8791 - val_loss: 0.2383 - val_binary_accuracy: 0.9275\n",
      "Epoch 186/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3835 - binary_accuracy: 0.8681 - val_loss: 0.2405 - val_binary_accuracy: 0.9275\n",
      "Epoch 187/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3811 - binary_accuracy: 0.8755 - val_loss: 0.2483 - val_binary_accuracy: 0.9275\n",
      "Epoch 188/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3809 - binary_accuracy: 0.8755 - val_loss: 0.2447 - val_binary_accuracy: 0.9275\n",
      "Epoch 189/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3797 - binary_accuracy: 0.8755 - val_loss: 0.2389 - val_binary_accuracy: 0.9275\n",
      "Epoch 190/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3809 - binary_accuracy: 0.8791 - val_loss: 0.2376 - val_binary_accuracy: 0.9275\n",
      "Epoch 191/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3810 - binary_accuracy: 0.8828 - val_loss: 0.2459 - val_binary_accuracy: 0.9420\n",
      "Epoch 192/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3790 - binary_accuracy: 0.8791 - val_loss: 0.2393 - val_binary_accuracy: 0.9275\n",
      "Epoch 193/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3808 - binary_accuracy: 0.8755 - val_loss: 0.2376 - val_binary_accuracy: 0.9275\n",
      "Epoch 194/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3789 - binary_accuracy: 0.8755 - val_loss: 0.2446 - val_binary_accuracy: 0.9420\n",
      "Epoch 195/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3786 - binary_accuracy: 0.8791 - val_loss: 0.2387 - val_binary_accuracy: 0.9275\n",
      "Epoch 196/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3804 - binary_accuracy: 0.8755 - val_loss: 0.2360 - val_binary_accuracy: 0.9275\n",
      "Epoch 197/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3764 - binary_accuracy: 0.8755 - val_loss: 0.2481 - val_binary_accuracy: 0.9420\n",
      "Epoch 198/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3808 - binary_accuracy: 0.8755 - val_loss: 0.2429 - val_binary_accuracy: 0.9275\n",
      "Epoch 199/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3793 - binary_accuracy: 0.8718 - val_loss: 0.2367 - val_binary_accuracy: 0.9275\n",
      "Epoch 200/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3765 - binary_accuracy: 0.8791 - val_loss: 0.2472 - val_binary_accuracy: 0.9420\n",
      "Epoch 201/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3779 - binary_accuracy: 0.8755 - val_loss: 0.2407 - val_binary_accuracy: 0.9275\n",
      "Epoch 202/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3787 - binary_accuracy: 0.8791 - val_loss: 0.2409 - val_binary_accuracy: 0.9275\n",
      "Epoch 203/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3782 - binary_accuracy: 0.8791 - val_loss: 0.2415 - val_binary_accuracy: 0.9275\n",
      "Epoch 204/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3780 - binary_accuracy: 0.8755 - val_loss: 0.2352 - val_binary_accuracy: 0.9275\n",
      "Epoch 205/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3829 - binary_accuracy: 0.8718 - val_loss: 0.2470 - val_binary_accuracy: 0.9275\n",
      "Epoch 206/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3754 - binary_accuracy: 0.8755 - val_loss: 0.2347 - val_binary_accuracy: 0.9275\n",
      "Epoch 207/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3812 - binary_accuracy: 0.8718 - val_loss: 0.2398 - val_binary_accuracy: 0.9275\n",
      "Epoch 208/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3846 - binary_accuracy: 0.8571 - val_loss: 0.2507 - val_binary_accuracy: 0.9130\n",
      "Epoch 209/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3788 - binary_accuracy: 0.8681 - val_loss: 0.2336 - val_binary_accuracy: 0.9275\n",
      "Epoch 210/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3763 - binary_accuracy: 0.8755 - val_loss: 0.2503 - val_binary_accuracy: 0.9130\n",
      "Epoch 211/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3766 - binary_accuracy: 0.8755 - val_loss: 0.2400 - val_binary_accuracy: 0.9275\n",
      "Epoch 212/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3753 - binary_accuracy: 0.8791 - val_loss: 0.2390 - val_binary_accuracy: 0.9275\n",
      "Epoch 213/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3761 - binary_accuracy: 0.8791 - val_loss: 0.2414 - val_binary_accuracy: 0.9130\n",
      "Epoch 214/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3754 - binary_accuracy: 0.8791 - val_loss: 0.2435 - val_binary_accuracy: 0.9420\n",
      "Epoch 215/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3751 - binary_accuracy: 0.8755 - val_loss: 0.2406 - val_binary_accuracy: 0.9130\n",
      "Epoch 216/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3742 - binary_accuracy: 0.8755 - val_loss: 0.2391 - val_binary_accuracy: 0.9275\n",
      "Epoch 217/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3743 - binary_accuracy: 0.8791 - val_loss: 0.2383 - val_binary_accuracy: 0.9275\n",
      "Epoch 218/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3762 - binary_accuracy: 0.8791 - val_loss: 0.2441 - val_binary_accuracy: 0.9275\n",
      "Epoch 219/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3759 - binary_accuracy: 0.8791 - val_loss: 0.2408 - val_binary_accuracy: 0.9130\n",
      "Epoch 220/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3728 - binary_accuracy: 0.8791 - val_loss: 0.2471 - val_binary_accuracy: 0.9130\n",
      "Epoch 221/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3738 - binary_accuracy: 0.8755 - val_loss: 0.2413 - val_binary_accuracy: 0.9130\n",
      "Epoch 222/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3722 - binary_accuracy: 0.8755 - val_loss: 0.2380 - val_binary_accuracy: 0.9275\n",
      "Epoch 223/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3751 - binary_accuracy: 0.8755 - val_loss: 0.2414 - val_binary_accuracy: 0.9130\n",
      "Epoch 224/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3737 - binary_accuracy: 0.8755 - val_loss: 0.2391 - val_binary_accuracy: 0.9130\n",
      "Epoch 225/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3718 - binary_accuracy: 0.8755 - val_loss: 0.2437 - val_binary_accuracy: 0.9130\n",
      "Epoch 226/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3765 - binary_accuracy: 0.8718 - val_loss: 0.2465 - val_binary_accuracy: 0.9275\n",
      "Epoch 227/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3719 - binary_accuracy: 0.8755 - val_loss: 0.2384 - val_binary_accuracy: 0.9130\n",
      "Epoch 228/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3739 - binary_accuracy: 0.8755 - val_loss: 0.2385 - val_binary_accuracy: 0.9130\n",
      "Epoch 229/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3716 - binary_accuracy: 0.8755 - val_loss: 0.2424 - val_binary_accuracy: 0.9130\n",
      "Epoch 230/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3710 - binary_accuracy: 0.8718 - val_loss: 0.2422 - val_binary_accuracy: 0.9130\n",
      "Epoch 231/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3718 - binary_accuracy: 0.8718 - val_loss: 0.2404 - val_binary_accuracy: 0.9130\n",
      "Epoch 232/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3712 - binary_accuracy: 0.8791 - val_loss: 0.2400 - val_binary_accuracy: 0.9130\n",
      "Epoch 233/500\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.3711 - binary_accuracy: 0.8718 - val_loss: 0.2464 - val_binary_accuracy: 0.9130\n",
      "Epoch 234/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3715 - binary_accuracy: 0.8755 - val_loss: 0.2404 - val_binary_accuracy: 0.9130\n",
      "Epoch 235/500\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3731 - binary_accuracy: 0.8791 - val_loss: 0.2462 - val_binary_accuracy: 0.9130\n",
      "Epoch 236/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3775 - binary_accuracy: 0.8755 - val_loss: 0.2370 - val_binary_accuracy: 0.9275\n",
      "Epoch 237/500\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.3807 - binary_accuracy: 0.8755 - val_loss: 0.2513 - val_binary_accuracy: 0.9275\n",
      "Epoch 238/500\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.3711 - binary_accuracy: 0.8791 - val_loss: 0.2349 - val_binary_accuracy: 0.9275\n",
      "Epoch 239/500\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.3735 - binary_accuracy: 0.8755 - val_loss: 0.2451 - val_binary_accuracy: 0.9130\n",
      "Epoch 240/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3702 - binary_accuracy: 0.8755 - val_loss: 0.2417 - val_binary_accuracy: 0.9130\n",
      "Epoch 241/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3716 - binary_accuracy: 0.8755 - val_loss: 0.2399 - val_binary_accuracy: 0.9130\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x261c36e7750>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(4, input_shape=(df_train_enc.shape[1],), activation='elu', kernel_initializer='he_uniform'),\n",
    "    keras.layers.Dense(4, activation='swish', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(4, activation='relu', kernel_initializer='he_uniform'),\n",
    "    keras.layers.Dense(4, activation='swish', kernel_initializer='glorot_uniform'),\n",
    "    keras.layers.Dense(4, activation='sigmoid', kernel_initializer='normal'),\n",
    "    keras.layers.Dense(1, activation='hard_sigmoid', kernel_initializer='he_normal'),\n",
    "])\n",
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "callback = EarlyStopping(monitor='binary_accuracy', patience=50, restore_best_weights=True)\n",
    "\n",
    "model.fit(df_train_enc, y_train, validation_data = (df_test_enc, y_test), epochs = 500, callbacks = [callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "No Extraccion       0.92      0.92      0.92        26\n",
      "   Extraccion       0.95      0.95      0.95        43\n",
      "\n",
      "     accuracy                           0.94        69\n",
      "    macro avg       0.94      0.94      0.94        69\n",
      " weighted avg       0.94      0.94      0.94        69\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_names = ['No Extraccion', 'Extraccion']\n",
    "y_pred = model.predict(df_test_enc)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm_df = pd.DataFrame(cm, index=target_names, columns=target_names)\n",
    "sns.set(font_scale=1.0)\n",
    "\n",
    "sns.heatmap(cm_df, cmap='Blues',annot=True, fmt='g')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_model(activation_1, activation_2, activation_3, activation_4, activation_5, activation_6, activation_7, activation_8,\n",
    "                 activation_9, activation_10,  init_mode_1, init_mode_2, init_mode_3, init_mode_4, init_mode_5, init_mode_6, \n",
    "                init_mode_7, init_mode_8, init_mode_9, init_mode_10, optimizer):\n",
    "\n",
    "    input_enc = keras.layers.Input(shape = (df_train_enc.shape[1],))\n",
    "    enc = keras.layers.Dense(4, activation=activation_1, kernel_initializer=init_mode_1)(input_enc)\n",
    "    enc = keras.Sequential([\n",
    "        keras.layers.Dense(4, activation=activation_2, kernel_initializer=init_mode_2),\n",
    "        keras.layers.Dense(4, activation=activation_3, kernel_initializer=init_mode_3),\n",
    "        keras.layers.Dense(4, activation=activation_4, kernel_initializer=init_mode_4),\n",
    "        keras.layers.Dense(4, activation=activation_5, kernel_initializer=init_mode_5),\n",
    "        keras.layers.Dense(2)\n",
    "    ])(enc)\n",
    "\n",
    "    input_dec = keras.layers.Input(shape = (2,))\n",
    "    dec = keras.layers.Dense(4, activation=activation_6, kernel_initializer=init_mode_6)(input_dec)\n",
    "    dec  = keras.Sequential([\n",
    "        keras.layers.Dense(4, activation=activation_7, kernel_initializer=init_mode_7),\n",
    "        keras.layers.Dense(4, activation=activation_8, kernel_initializer=init_mode_8),\n",
    "        keras.layers.Dense(4, activation=activation_9, kernel_initializer=init_mode_9),\n",
    "        keras.layers.Dense(4, activation=activation_10, kernel_initializer=init_mode_10),\n",
    "        keras.layers.Dense(df_train_enc.shape[1])\n",
    "    ])(dec)\n",
    "    \n",
    "    encoder = keras.models.Model(input_enc, enc)\n",
    "    decoder = keras.models.Model(input_dec, dec)\n",
    "\n",
    "    autoencoder_input = keras.layers.Input(shape=(df_train_enc.shape[1],))\n",
    "    encoded_repr = encoder(autoencoder_input)\n",
    "    reconstructed = decoder(encoded_repr)\n",
    "\n",
    "    autoencoder = keras.models.Model(autoencoder_input, reconstructed)\n",
    "\n",
    "    autoencoder.compile(optimizer=optimizer,\n",
    "                  loss=keras.losses.MeanSquaredError(),\n",
    "                  metrics=[keras.metrics.MeanAbsoluteError(),])\n",
    "    return autoencoder\n",
    "\n",
    "# Función objetivo para Optuna\n",
    "def objective(trial):\n",
    "    activations_list = [\"elu\",\"exponential\",\"hard_sigmoid\",\"linear\", \"relu\",\"selu\",\"sigmoid\",\"softmax\",\"softplus\",\n",
    "                                                                                         \"softsign\",\"swish\",\"tanh\"]\n",
    "    activation_1 = trial.suggest_categorical('activation_1', activations_list)\n",
    "    activation_2 = trial.suggest_categorical('activation_2', activations_list)\n",
    "    activation_3 = trial.suggest_categorical('activation_3', activations_list)\n",
    "    activation_4 = trial.suggest_categorical('activation_4', activations_list)\n",
    "    activation_5 = trial.suggest_categorical('activation_5', activations_list)\n",
    "    activation_6 = trial.suggest_categorical('activation_6', activations_list)\n",
    "    activation_7 = trial.suggest_categorical('activation_7', activations_list)\n",
    "    activation_8 = trial.suggest_categorical('activation_8', activations_list)\n",
    "    activation_9 = trial.suggest_categorical('activation_9', activations_list)\n",
    "    activation_10 = trial.suggest_categorical('activation_10', activations_list)\n",
    "    \n",
    "    init_list = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "    init_mode_1 = trial.suggest_categorical('init_mode_1', init_list)\n",
    "    init_mode_2 = trial.suggest_categorical('init_mode_2', init_list)\n",
    "    init_mode_3 = trial.suggest_categorical('init_mode_3', init_list)\n",
    "    init_mode_4 = trial.suggest_categorical('init_mode_4', init_list)\n",
    "    init_mode_5 = trial.suggest_categorical('init_mode_5', init_list)\n",
    "    init_mode_6 = trial.suggest_categorical('init_mode_6', init_list)\n",
    "    init_mode_7 = trial.suggest_categorical('init_mode_7', init_list)\n",
    "    init_mode_8 = trial.suggest_categorical('init_mode_8', init_list)\n",
    "    init_mode_9 = trial.suggest_categorical('init_mode_9', init_list)\n",
    "    init_mode_10 = trial.suggest_categorical('init_mode_10', init_list)\n",
    "    \n",
    "    optimizer = trial.suggest_categorical('optimizer', ['SGD', 'RMSprop', 'Adam', 'Adadelta', 'Adagrad', 'Adamax', \n",
    "                                                        'Nadam', 'Ftrl'])\n",
    "    model = create_model(activation_1, activation_2, activation_3, activation_4, activation_5, activation_6, activation_7, activation_8,\n",
    "                 activation_9, activation_10,  init_mode_1, init_mode_2, init_mode_3, init_mode_4, init_mode_5, init_mode_6, \n",
    "                init_mode_7, init_mode_8, init_mode_9, init_mode_10, optimizer)\n",
    "    \n",
    "    history = model.fit(df_train_enc, df_train_enc, epochs=10, validation_split=0.1, verbose=0)\n",
    "    \n",
    "    # Usa la pérdida de validación como métrica para optimizar\n",
    "    val_loss = history.history['mean_absolute_error'][-1]\n",
    "    return val_loss\n",
    "\n",
    "# Iniciar la optimización con Optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=500)\n",
    "\n",
    "print(\"Mejores hiperparámetros:\", study.best_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejores hiperparámetros: {'activation_1': 'swish', 'activation_2': 'elu', 'activation_3': 'linear', 'activation_4': 'selu', 'activation_5': 'selu', 'activation_6': 'exponential', 'activation_7': 'exponential', 'activation_8': 'relu', 'activation_9': 'selu', 'activation_10': 'selu', 'init_mode_1': 'normal', 'init_mode_2': 'normal', 'init_mode_3': 'he_uniform', 'init_mode_4': 'he_uniform', 'init_mode_5': 'glorot_normal', 'init_mode_6': 'uniform', 'init_mode_7': 'he_uniform', 'init_mode_8': 'he_normal', 'init_mode_9': 'zero', 'init_mode_10': 'glorot_normal', 'optimizer': 'RMSprop'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_enc = keras.layers.Input(shape = (x_df.shape[1],))\n",
    "enc = keras.layers.Dense(3, activation='elu', kernel_initializer='he_normal')(input_enc)\n",
    "enc = keras.Sequential([\n",
    "    keras.layers.Dense(3, activation='elu', kernel_initializer='he_uniform'),\n",
    "    keras.layers.Dense(3, activation='elu', kernel_initializer='lecun_uniform'),\n",
    "    keras.layers.Dense(3, activation='swish', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(3, activation='swish', kernel_initializer='normal'),\n",
    "    keras.layers.Dense(3)\n",
    "])(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dec = keras.layers.Input(shape = (3,))\n",
    "dec = keras.layers.Dense(3, activation='swish', kernel_initializer='normal')(input_dec)\n",
    "dec  = keras.Sequential([\n",
    "    keras.layers.Dense(3, activation='swish', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(3, activation='tanh', kernel_initializer='normal'),\n",
    "    keras.layers.Dense(3, activation='elu', kernel_initializer='lecun_uniform'),\n",
    "    keras.layers.Dense(3, activation='elu', kernel_initializer='he_uniform'),\n",
    "    keras.layers.Dense(3)\n",
    "])(dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = keras.models.Model(input_enc, enc)\n",
    "decoder = keras.models.Model(input_dec, dec)\n",
    "\n",
    "autoencoder_input = keras.layers.Input(shape=(3,))\n",
    "encoded_repr = encoder(autoencoder_input)\n",
    "reconstructed = decoder(encoded_repr)\n",
    "\n",
    "autoencoder = keras.models.Model(autoencoder_input, reconstructed)\n",
    "\n",
    "autoencoder.compile(\n",
    "    optimizer='Adamax',\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    metrics=[keras.metrics.MeanAbsoluteError(),]\n",
    ")\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='mean_absolute_error', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(x_df,  y_df, epochs=10, validation_data=(x_test, y_test),callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['No Extraccion', 'Extraccion']\n",
    "y_pred = model.predict(df_test_enc)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm_df = pd.DataFrame(cm, index=target_names, columns=target_names)\n",
    "sns.set(font_scale=1.0)\n",
    "\n",
    "sns.heatmap(cm_df, cmap='Blues',annot=True, fmt='g')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyMmFJMJnfr+xQ1ws9U5ce7A",
   "collapsed_sections": [
    "Jfz1nZKW5j8m",
    "S4mIdIGe5qSA",
    "TGXrlRDt59eC",
    "lTQErm586Rp5",
    "BUMHf5Jl7MAg",
    "x3VAYtS1JhY9"
   ],
   "mount_file_id": "1N8-CJAwZrYqN0gK7v7GFblWvUSnzS1b0",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
